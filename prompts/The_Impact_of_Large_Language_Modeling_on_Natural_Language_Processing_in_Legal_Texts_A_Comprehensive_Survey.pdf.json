[{"reference": "AraLegal-BERT A pretrained language model for Arabic Legal text", "title": "AraLegal-BERT: A pretrained language model for Arabic Legal text", "abstract": "The effectiveness of the BERT model on multiple linguistic tasks has been\nwell documented. On the other hand, its potentials for narrow and specific\ndomains such as Legal, have not been fully explored. In this paper, we examine\nhow BERT can be used in the Arabic legal domain and try customizing this\nlanguage model for several downstream tasks using several different\ndomain-relevant training and testing datasets to train BERT from scratch. We\nintroduce the AraLegal-BERT, a bidirectional encoder Transformer-based model\nthat have been thoroughly tested and carefully optimized with the goal to\namplify the impact of NLP-driven solution concerning jurisprudence, legal\ndocuments, and legal practice. We fine-tuned AraLegal-BERT and evaluated it\nagainst three BERT variations for Arabic language in three natural languages\nunderstanding (NLU) tasks. The results show that the base version of\nAraLegal-BERT achieve better accuracy than the general and original BERT over\nthe Legal text.", "url": "http://arxiv.org/abs/2210.08284v1"}, {"reference": "PromptSource An Integrated Development Environment and Repository for Natural Language Prompts", "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts", "abstract": "PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.", "url": "http://arxiv.org/abs/2202.01279v3"}, {"reference": "A Survey on Legal Judgment Prediction Datasets Metrics Models and Challenges", "title": "A Survey on Legal Judgment Prediction: Datasets, Metrics, Models and Challenges", "abstract": "Legal judgment prediction (LJP) applies Natural Language Processing (NLP)\ntechniques to predict judgment results based on fact descriptions\nautomatically. Recently, large-scale public datasets and advances in NLP\nresearch have led to increasing interest in LJP. Despite a clear gap between\nmachine and human performance, impressive results have been achieved in various\nbenchmark datasets. In this paper, to address the current lack of comprehensive\nsurvey of existing LJP tasks, datasets, models and evaluations, (1) we analyze\n31 LJP datasets in 6 languages, present their construction process and define a\nclassification method of LJP with 3 different attributes; (2) we summarize 14\nevaluation metrics under four categories for different outputs of LJP tasks;\n(3) we review 12 legal-domain pretrained models in 3 languages and highlight 3\nmajor research directions for LJP; (4) we show the state-of-art results for 8\nrepresentative datasets from different court cases and discuss the open\nchallenges. This paper can provide up-to-date and comprehensive reviews to help\nreaders understand the status of LJP. We hope to facilitate both NLP\nresearchers and legal professionals for further joint efforts in this problem.", "url": "http://arxiv.org/abs/2204.04859v1"}, {"reference": "Bert Pre-training of deep bidirectional transformers for language understanding", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).", "url": "http://arxiv.org/abs/1810.04805v2"}, {"reference": "State of the Art in Artificial Intelligence applied to the Legal Domain", "title": "State of the Art in Artificial Intelligence applied to the Legal Domain", "abstract": "While Artificial Intelligence applied to the legal domain is a topic with\norigins in the last century, recent advances in Artificial Intelligence are\nposed to revolutionize it. This work presents an overview and contextualizes\nthe main advances on the field of Natural Language Processing and how these\nadvances have been used to further the state of the art in legal text analysis.", "url": "http://arxiv.org/abs/2204.07047v1"}, {"reference": "Towards WinoQueer Developing a Benchmark for Anti-Queer Bias in Large Language Models", "title": "Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models", "abstract": "This paper presents exploratory work on whether and to what extent biases\nagainst queer and trans people are encoded in large language models (LLMs) such\nas BERT. We also propose a method for reducing these biases in downstream\ntasks: finetuning the models on data written by and/or about queer people. To\nmeasure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,\nmodeled after other bias-detection benchmarks but addressing homophobic and\ntransphobic biases. We found that BERT shows significant homophobic bias, but\nthis bias can be mostly mitigated by finetuning BERT on a natural language\ncorpus written by members of the LGBTQ+ community.", "url": "http://arxiv.org/abs/2206.11484v2"}, {"reference": "Natural Language Processing in the Legal Domain", "title": "Natural Language Processing in the Legal Domain", "abstract": "In this paper, we summarize the current state of the field of NLP & Law with\na specific focus on recent technical and substantive developments. To support\nour analysis, we construct and analyze a nearly complete corpus of more than\nsix hundred NLP & Law related papers published over the past decade. Our\nanalysis highlights several major trends. Namely, we document an increasing\nnumber of papers written, tasks undertaken, and languages covered over the\ncourse of the past decade. We observe an increase in the sophistication of the\nmethods which researchers deployed in this applied context. Slowly but surely,\nLegal NLP is beginning to match not only the methodological sophistication of\ngeneral NLP but also the professional standards of data availability and code\nreproducibility observed within the broader scientific community. We believe\nall of these trends bode well for the future of the field, but many questions\nin both the academic and commercial sphere still remain open.", "url": "http://arxiv.org/abs/2302.12039v1"}, {"reference": "Crosslingual Generalization through Multitask Finetuning", "title": "Crosslingual Generalization through Multitask Finetuning", "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\nof MTF have focused on English data and models. We apply MTF to the pretrained\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\nBLOOMZ and mT0. We find finetuning large multilingual language models on\nEnglish tasks with English prompts allows for task generalization to\nnon-English languages that appear only in the pretraining corpus. Finetuning on\nmultilingual tasks with English prompts further improves performance on English\nand non-English tasks leading to various state-of-the-art zero-shot results. We\nalso investigate finetuning on multilingual tasks with prompts that have been\nmachine-translated from English to match the language of each dataset. We find\ntraining on these machine-translated prompts leads to better performance on\nhuman-written prompts in the respective languages. Surprisingly, we find models\nare capable of zero-shot generalization to tasks in languages they have never\nintentionally seen. We conjecture that the models are learning higher-level\ncapabilities that are both task- and language-agnostic. In addition, we\nintroduce xP3, a composite of supervised datasets in 46 languages with English\nand machine-translated prompts. Our code, datasets and models are freely\navailable at https://github.com/bigscience-workshop/xmtf.", "url": "http://arxiv.org/abs/2211.01786v2"}, {"reference": "Law Informs Code A Legal Informatics Approach to Aligning Artificial Intelligence with Humans", "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans", "abstract": "We are currently unable to specify human goals and societal values in a way\nthat reliably directs AI behavior. Law-making and legal interpretation form a\ncomputational engine that converts opaque human values into legible directives.\n\"Law Informs Code\" is the research agenda embedding legal knowledge and\nreasoning in AI. Similar to how parties to a legal contract cannot foresee\nevery potential contingency of their future relationship, and legislators\ncannot predict all the circumstances under which their proposed bills will be\napplied, we cannot ex ante specify rules that provably direct good AI behavior.\nLegal theory and practice have developed arrays of tools to address these\nspecification problems. For instance, legal standards allow humans to develop\nshared understandings and adapt them to novel situations. In contrast to more\nprosaic uses of the law (e.g., as a deterrent of bad behavior through the\nthreat of sanction), leveraged as an expression of how humans communicate their\ngoals, and what society values, Law Informs Code.\n  We describe how data generated by legal processes (methods of law-making,\nstatutory interpretation, contract drafting, applications of legal standards,\nlegal reasoning, etc.) can facilitate the robust specification of inherently\nvague human goals. This increases human-AI alignment and the local usefulness\nof AI. Toward society-AI alignment, we present a framework for understanding\nlaw as the applied philosophy of multi-agent alignment. Although law is partly\na reflection of historically contingent political power - and thus not a\nperfect aggregation of citizen preferences - if properly parsed, its\ndistillation offers the most legitimate computational comprehension of societal\nvalues available. If law eventually informs powerful AI, engaging in the\ndeliberative political process to improve law takes on even more meaning.", "url": "http://arxiv.org/abs/2209.13020v14"}, {"reference": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs", "title": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs", "abstract": "The analysis of public affairs documents is crucial for citizens as it\npromotes transparency, accountability, and informed decision-making. It allows\ncitizens to understand government policies, participate in public discourse,\nand hold representatives accountable. This is crucial, and sometimes a matter\nof life or death, for companies whose operation depend on certain regulations.\nLarge Language Models (LLMs) have the potential to greatly enhance the analysis\nof public affairs documents by effectively processing and understanding the\ncomplex language used in such documents. In this work, we analyze the\nperformance of LLMs in classifying public affairs documents. As a natural\nmulti-label task, the classification of these documents presents important\nchallenges. In this work, we use a regex-powered tool to collect a database of\npublic affairs documents with more than 33K samples and 22.5M tokens. Our\nexperiments assess the performance of 4 different Spanish LLMs to classify up\nto 30 different topics in the data in different configurations. The results\nshows that LLMs can be of great use to process domain-specific documents, such\nas those in the domain of public affairs.", "url": "http://arxiv.org/abs/2306.02864v2"}, {"reference": "Learning Transferable Visual Models From Natural Language Supervision", "title": "Learning Transferable Visual Models From Natural Language Supervision", "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set\nof predetermined object categories. This restricted form of supervision limits\ntheir generality and usability since additional labeled data is needed to\nspecify any other visual concept. Learning directly from raw text about images\nis a promising alternative which leverages a much broader source of\nsupervision. We demonstrate that the simple pre-training task of predicting\nwhich caption goes with which image is an efficient and scalable way to learn\nSOTA image representations from scratch on a dataset of 400 million (image,\ntext) pairs collected from the internet. After pre-training, natural language\nis used to reference learned visual concepts (or describe new ones) enabling\nzero-shot transfer of the model to downstream tasks. We study the performance\nof this approach by benchmarking on over 30 different existing computer vision\ndatasets, spanning tasks such as OCR, action recognition in videos,\ngeo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a\nfully supervised baseline without the need for any dataset specific training.\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28 million training examples it\nwas trained on. We release our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP.", "url": "http://arxiv.org/abs/2103.00020v1"}, {"reference": "Exploring the limits of transfer learning with a unified text-to-text transformer", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.", "url": "http://arxiv.org/abs/1910.10683v4"}, {"reference": "Billions of parameters are worth more than in-domain training data A case study in the legal case entailment task", "title": "Billions of Parameters Are Worth More Than In-domain Training Data: A case study in the Legal Case Entailment Task", "abstract": "Recent work has shown that language models scaled to billions of parameters,\nsuch as GPT-3, perform remarkably well in zero-shot and few-shot scenarios. In\nthis work, we experiment with zero-shot models in the legal case entailment\ntask of the COLIEE 2022 competition. Our experiments show that scaling the\nnumber of parameters in a language model improves the F1 score of our previous\nzero-shot result by more than 6 points, suggesting that stronger zero-shot\ncapability may be a characteristic of larger models, at least for this task.\nOur 3B-parameter zero-shot model outperforms all models, including ensembles,\nin the COLIEE 2021 test set and also achieves the best performance of a single\nmodel in the COLIEE 2022 competition, second only to the ensemble composed of\nthe 3B model itself and a smaller version of the same model. Despite the\nchallenges posed by large language models, mainly due to latency constraints in\nreal-time applications, we provide a demonstration of our zero-shot monoT5-3b\nmodel being used in production as a search engine, including for legal\ndocuments. The code for our submission and the demo of our system are available\nat https://github.com/neuralmind-ai/coliee and\nhttps://neuralsearchx.neuralmind.ai, respectively.", "url": "http://arxiv.org/abs/2205.15172v1"}, {"reference": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "abstract": "Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\nhypothesized that this is a consequence of implicit multitask learning in\nlanguage models' pretraining (Radford et al., 2019). Can zero-shot\ngeneralization instead be directly induced by explicit multitask learning? To\ntest this question at scale, we develop a system for easily mapping any natural\nlanguage tasks into a human-readable prompted form. We convert a large set of\nsupervised datasets, each with multiple prompts with diverse wording. These\nprompted datasets allow for benchmarking the ability of a model to perform\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\nwide variety of tasks. The model attains strong zero-shot performance on\nseveral standard datasets, often outperforming models up to 16x its size.\nFurther, our approach attains strong performance on a subset of tasks from the\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\nare available at https://github.com/bigscience-workshop/promptsource.", "url": "http://arxiv.org/abs/2110.08207v3"}, {"reference": "Bloom A 176b-parameter openaccess multilingual language model", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.", "url": "http://arxiv.org/abs/2211.05100v4"}, {"reference": "Text Classification via Large Language Models", "title": "Text Classification via Large Language Models", "abstract": "Despite the remarkable success of large-scale Language Models (LLMs) such as\nGPT-3, their performances still significantly underperform fine-tuned models in\nthe task of text classification. This is due to (1) the lack of reasoning\nability in addressing complex linguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of tokens allowed in in-context\nlearning.\n  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to addressing the complex linguistic\nphenomena involved in text classification: CARP first prompts LLMs to find\nsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),\nbased on which a diagnostic reasoning process is induced for final decisions.\nTo further address the limited-token issue, CARP uses a fine-tuned model on the\nsupervised dataset for $k$NN demonstration search in the in-context learning,\nallowing the model to take the advantage of both LLM's generalization ability\nand the task-specific evidence provided by the full labeled dataset.\nRemarkably, CARP yields new SOTA performances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on\nAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP\ndelivers impressive abilities on low-resource and domain-adaptation setups.\nSpecifically, using 16 examples per class, CARP achieves comparable\nperformances to supervised models with 1,024 examples per class.", "url": "http://arxiv.org/abs/2305.08377v3"}, {"reference": "Attention Is All You Need", "title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.", "url": "http://arxiv.org/abs/1706.03762v7"}, {"reference": "mT5 A Massively Multilingual Pretrained Text-to-Text Transformer", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified\ntext-to-text format and scale to attain state-of-the-art results on a wide\nvariety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based\ndataset covering 101 languages. We detail the design and modified training of\nmT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent \"accidental\ntranslation\" in the zero-shot setting, where a generative model chooses to\n(partially) translate its prediction into the wrong language. All of the code\nand model checkpoints used in this work are publicly available.", "url": "http://arxiv.org/abs/2010.11934v3"}]