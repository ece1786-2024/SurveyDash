[
  {
    "reference": "AraLegal-BERT A pretrained language model for Arabic Legal text",
    "title": "AraLegal-BERT: A pretrained language model for Arabic Legal text",
    "authors": [
      "Muhammad AL-Qurishi",
      "Sarah AlQaseemi",
      "Riad Soussi"
    ],
    "year": 2022,
    "abstract": "The effectiveness of the BERT model on multiple linguistic tasks has been\nwell documented. On the other hand, its potentials for narrow and specific\ndomains such as Legal, have not been fully explored. In this paper, we examine\nhow BERT can be used in the Arabic legal domain and try customizing this\nlanguage model for several downstream tasks using several different\ndomain-relevant training and testing datasets to train BERT from scratch. We\nintroduce the AraLegal-BERT, a bidirectional encoder Transformer-based model\nthat have been thoroughly tested and carefully optimized with the goal to\namplify the impact of NLP-driven solution concerning jurisprudence, legal\ndocuments, and legal practice. We fine-tuned AraLegal-BERT and evaluated it\nagainst three BERT variations for Arabic language in three natural languages\nunderstanding (NLU) tasks. The results show that the base version of\nAraLegal-BERT achieve better accuracy than the general and original BERT over\nthe Legal text.",
    "url": "http://arxiv.org/abs/2210.08284v1"
  },
  {
    "reference": "PromptSource An Integrated Development Environment and Repository for Natural Language Prompts",
    "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
    "authors": [
      "Stephen H. Bach",
      "Victor Sanh",
      "Zheng-Xin Yong",
      "Albert Webson",
      "Colin Raffel",
      "Nihal V. Nayak",
      "Abheesht Sharma",
      "Taewoon Kim",
      "M Saiful Bari",
      "Thibault Fevry",
      "Zaid Alyafeai",
      "Manan Dey",
      "Andrea Santilli",
      "Zhiqing Sun",
      "Srulik Ben-David",
      "Canwen Xu",
      "Gunjan Chhablani",
      "Han Wang",
      "Jason Alan Fries",
      "Maged S. Al-shaibani",
      "Shanya Sharma",
      "Urmish Thakker",
      "Khalid Almubarak",
      "Xiangru Tang",
      "Dragomir Radev",
      "Mike Tian-Jian Jiang",
      "Alexander M. Rush"
    ],
    "year": 2022,
    "abstract": "PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.",
    "url": "http://arxiv.org/abs/2202.01279v3"
  },
  {
    "reference": "A Survey on Legal Judgment Prediction Datasets Metrics Models and Challenges",
    "title": "A Survey on Legal Judgment Prediction: Datasets, Metrics, Models and Challenges",
    "authors": [
      "Junyun Cui",
      "Xiaoyu Shen",
      "Feiping Nie",
      "Zheng Wang",
      "Jinglong Wang",
      "Yulong Chen"
    ],
    "year": 2022,
    "abstract": "Legal judgment prediction (LJP) applies Natural Language Processing (NLP)\ntechniques to predict judgment results based on fact descriptions\nautomatically. Recently, large-scale public datasets and advances in NLP\nresearch have led to increasing interest in LJP. Despite a clear gap between\nmachine and human performance, impressive results have been achieved in various\nbenchmark datasets. In this paper, to address the current lack of comprehensive\nsurvey of existing LJP tasks, datasets, models and evaluations, (1) we analyze\n31 LJP datasets in 6 languages, present their construction process and define a\nclassification method of LJP with 3 different attributes; (2) we summarize 14\nevaluation metrics under four categories for different outputs of LJP tasks;\n(3) we review 12 legal-domain pretrained models in 3 languages and highlight 3\nmajor research directions for LJP; (4) we show the state-of-art results for 8\nrepresentative datasets from different court cases and discuss the open\nchallenges. This paper can provide up-to-date and comprehensive reviews to help\nreaders understand the status of LJP. We hope to facilitate both NLP\nresearchers and legal professionals for further joint efforts in this problem.",
    "url": "http://arxiv.org/abs/2204.04859v1"
  },
  {
    "reference": "Bert Pre-training of deep bidirectional transformers for language understanding",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "year": 2018,
    "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
    "url": "http://arxiv.org/abs/1810.04805v2"
  },
  {
    "reference": "State of the Art in Artificial Intelligence applied to the Legal Domain",
    "title": "State of the Art in Artificial Intelligence applied to the Legal Domain",
    "authors": [
      "Jo\u00e3o Dias",
      "Pedro A. Santos",
      "Nuno Cordeiro",
      "Ana Antunes",
      "Bruno Martins",
      "Jorge Baptista",
      "Carlos Gon\u00e7alves"
    ],
    "year": 2022,
    "abstract": "While Artificial Intelligence applied to the legal domain is a topic with\norigins in the last century, recent advances in Artificial Intelligence are\nposed to revolutionize it. This work presents an overview and contextualizes\nthe main advances on the field of Natural Language Processing and how these\nadvances have been used to further the state of the art in legal text analysis.",
    "url": "http://arxiv.org/abs/2204.07047v1"
  },
  {
    "reference": "Towards WinoQueer Developing a Benchmark for Anti-Queer Bias in Large Language Models",
    "title": "Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models",
    "authors": [
      "Virginia K. Felkner",
      "Ho-Chun Herbert Chang",
      "Eugene Jang",
      "Jonathan May"
    ],
    "year": 2022,
    "abstract": "This paper presents exploratory work on whether and to what extent biases\nagainst queer and trans people are encoded in large language models (LLMs) such\nas BERT. We also propose a method for reducing these biases in downstream\ntasks: finetuning the models on data written by and/or about queer people. To\nmeasure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,\nmodeled after other bias-detection benchmarks but addressing homophobic and\ntransphobic biases. We found that BERT shows significant homophobic bias, but\nthis bias can be mostly mitigated by finetuning BERT on a natural language\ncorpus written by members of the LGBTQ+ community.",
    "url": "http://arxiv.org/abs/2206.11484v2"
  },
  {
    "reference": "Natural Language Processing in the Legal Domain",
    "title": "Natural Language Processing in the Legal Domain",
    "authors": [
      "Daniel Martin Katz",
      "Dirk Hartung",
      "Lauritz Gerlach",
      "Abhik Jana",
      "Michael J. Bommarito II"
    ],
    "year": 2023,
    "abstract": "In this paper, we summarize the current state of the field of NLP & Law with\na specific focus on recent technical and substantive developments. To support\nour analysis, we construct and analyze a nearly complete corpus of more than\nsix hundred NLP & Law related papers published over the past decade. Our\nanalysis highlights several major trends. Namely, we document an increasing\nnumber of papers written, tasks undertaken, and languages covered over the\ncourse of the past decade. We observe an increase in the sophistication of the\nmethods which researchers deployed in this applied context. Slowly but surely,\nLegal NLP is beginning to match not only the methodological sophistication of\ngeneral NLP but also the professional standards of data availability and code\nreproducibility observed within the broader scientific community. We believe\nall of these trends bode well for the future of the field, but many questions\nin both the academic and commercial sphere still remain open.",
    "url": "http://arxiv.org/abs/2302.12039v1"
  },
  {
    "reference": "Crosslingual Generalization through Multitask Finetuning",
    "title": "Crosslingual Generalization through Multitask Finetuning",
    "authors": [
      "Niklas Muennighoff",
      "Thomas Wang",
      "Lintang Sutawika",
      "Adam Roberts",
      "Stella Biderman",
      "Teven Le Scao",
      "M Saiful Bari",
      "Sheng Shen",
      "Zheng-Xin Yong",
      "Hailey Schoelkopf",
      "Xiangru Tang",
      "Dragomir Radev",
      "Alham Fikri Aji",
      "Khalid Almubarak",
      "Samuel Albanie",
      "Zaid Alyafeai",
      "Albert Webson",
      "Edward Raff",
      "Colin Raffel"
    ],
    "year": 2022,
    "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\nof MTF have focused on English data and models. We apply MTF to the pretrained\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\nBLOOMZ and mT0. We find finetuning large multilingual language models on\nEnglish tasks with English prompts allows for task generalization to\nnon-English languages that appear only in the pretraining corpus. Finetuning on\nmultilingual tasks with English prompts further improves performance on English\nand non-English tasks leading to various state-of-the-art zero-shot results. We\nalso investigate finetuning on multilingual tasks with prompts that have been\nmachine-translated from English to match the language of each dataset. We find\ntraining on these machine-translated prompts leads to better performance on\nhuman-written prompts in the respective languages. Surprisingly, we find models\nare capable of zero-shot generalization to tasks in languages they have never\nintentionally seen. We conjecture that the models are learning higher-level\ncapabilities that are both task- and language-agnostic. In addition, we\nintroduce xP3, a composite of supervised datasets in 46 languages with English\nand machine-translated prompts. Our code, datasets and models are freely\navailable at https://github.com/bigscience-workshop/xmtf.",
    "url": "http://arxiv.org/abs/2211.01786v2"
  },
  {
    "reference": "Law Informs Code A Legal Informatics Approach to Aligning Artificial Intelligence with Humans",
    "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans",
    "authors": [
      "John J. Nay"
    ],
    "year": 2022,
    "abstract": "We are currently unable to specify human goals and societal values in a way\nthat reliably directs AI behavior. Law-making and legal interpretation form a\ncomputational engine that converts opaque human values into legible directives.\n\"Law Informs Code\" is the research agenda embedding legal knowledge and\nreasoning in AI. Similar to how parties to a legal contract cannot foresee\nevery potential contingency of their future relationship, and legislators\ncannot predict all the circumstances under which their proposed bills will be\napplied, we cannot ex ante specify rules that provably direct good AI behavior.\nLegal theory and practice have developed arrays of tools to address these\nspecification problems. For instance, legal standards allow humans to develop\nshared understandings and adapt them to novel situations. In contrast to more\nprosaic uses of the law (e.g., as a deterrent of bad behavior through the\nthreat of sanction), leveraged as an expression of how humans communicate their\ngoals, and what society values, Law Informs Code.\n  We describe how data generated by legal processes (methods of law-making,\nstatutory interpretation, contract drafting, applications of legal standards,\nlegal reasoning, etc.) can facilitate the robust specification of inherently\nvague human goals. This increases human-AI alignment and the local usefulness\nof AI. Toward society-AI alignment, we present a framework for understanding\nlaw as the applied philosophy of multi-agent alignment. Although law is partly\na reflection of historically contingent political power - and thus not a\nperfect aggregation of citizen preferences - if properly parsed, its\ndistillation offers the most legitimate computational comprehension of societal\nvalues available. If law eventually informs powerful AI, engaging in the\ndeliberative political process to improve law takes on even more meaning.",
    "url": "http://arxiv.org/abs/2209.13020v14"
  },
  {
    "reference": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs",
    "title": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs",
    "authors": [
      "Alejandro Pe\u00f1a",
      "Aythami Morales",
      "Julian Fierrez",
      "Ignacio Serna",
      "Javier Ortega-Garcia",
      "I\u00f1igo Puente",
      "Jorge Cordova",
      "Gonzalo Cordova"
    ],
    "year": 2023,
    "abstract": "The analysis of public affairs documents is crucial for citizens as it\npromotes transparency, accountability, and informed decision-making. It allows\ncitizens to understand government policies, participate in public discourse,\nand hold representatives accountable. This is crucial, and sometimes a matter\nof life or death, for companies whose operation depend on certain regulations.\nLarge Language Models (LLMs) have the potential to greatly enhance the analysis\nof public affairs documents by effectively processing and understanding the\ncomplex language used in such documents. In this work, we analyze the\nperformance of LLMs in classifying public affairs documents. As a natural\nmulti-label task, the classification of these documents presents important\nchallenges. In this work, we use a regex-powered tool to collect a database of\npublic affairs documents with more than 33K samples and 22.5M tokens. Our\nexperiments assess the performance of 4 different Spanish LLMs to classify up\nto 30 different topics in the data in different configurations. The results\nshows that LLMs can be of great use to process domain-specific documents, such\nas those in the domain of public affairs.",
    "url": "http://arxiv.org/abs/2306.02864v2"
  },
  {
    "reference": "Learning Transferable Visual Models From Natural Language Supervision",
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "authors": [
      "Alec Radford",
      "Jong Wook Kim",
      "Chris Hallacy",
      "Aditya Ramesh",
      "Gabriel Goh",
      "Sandhini Agarwal",
      "Girish Sastry",
      "Amanda Askell",
      "Pamela Mishkin",
      "Jack Clark",
      "Gretchen Krueger",
      "Ilya Sutskever"
    ],
    "year": 2021,
    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set\nof predetermined object categories. This restricted form of supervision limits\ntheir generality and usability since additional labeled data is needed to\nspecify any other visual concept. Learning directly from raw text about images\nis a promising alternative which leverages a much broader source of\nsupervision. We demonstrate that the simple pre-training task of predicting\nwhich caption goes with which image is an efficient and scalable way to learn\nSOTA image representations from scratch on a dataset of 400 million (image,\ntext) pairs collected from the internet. After pre-training, natural language\nis used to reference learned visual concepts (or describe new ones) enabling\nzero-shot transfer of the model to downstream tasks. We study the performance\nof this approach by benchmarking on over 30 different existing computer vision\ndatasets, spanning tasks such as OCR, action recognition in videos,\ngeo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a\nfully supervised baseline without the need for any dataset specific training.\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28 million training examples it\nwas trained on. We release our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP.",
    "url": "http://arxiv.org/abs/2103.00020v1"
  },
  {
    "reference": "Exploring the limits of transfer learning with a unified text-to-text transformer",
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "authors": [
      "Colin Raffel",
      "Noam Shazeer",
      "Adam Roberts",
      "Katherine Lee",
      "Sharan Narang",
      "Michael Matena",
      "Yanqi Zhou",
      "Wei Li",
      "Peter J. Liu"
    ],
    "year": 2019,
    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.",
    "url": "http://arxiv.org/abs/1910.10683v4"
  },
  {
    "reference": "Billions of parameters are worth more than in-domain training data A case study in the legal case entailment task",
    "title": "Billions of Parameters Are Worth More Than In-domain Training Data: A case study in the Legal Case Entailment Task",
    "authors": [
      "Guilherme Moraes Rosa",
      "Luiz Bonifacio",
      "Vitor Jeronymo",
      "Hugo Abonizio",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "year": 2022,
    "abstract": "Recent work has shown that language models scaled to billions of parameters,\nsuch as GPT-3, perform remarkably well in zero-shot and few-shot scenarios. In\nthis work, we experiment with zero-shot models in the legal case entailment\ntask of the COLIEE 2022 competition. Our experiments show that scaling the\nnumber of parameters in a language model improves the F1 score of our previous\nzero-shot result by more than 6 points, suggesting that stronger zero-shot\ncapability may be a characteristic of larger models, at least for this task.\nOur 3B-parameter zero-shot model outperforms all models, including ensembles,\nin the COLIEE 2021 test set and also achieves the best performance of a single\nmodel in the COLIEE 2022 competition, second only to the ensemble composed of\nthe 3B model itself and a smaller version of the same model. Despite the\nchallenges posed by large language models, mainly due to latency constraints in\nreal-time applications, we provide a demonstration of our zero-shot monoT5-3b\nmodel being used in production as a search engine, including for legal\ndocuments. The code for our submission and the demo of our system are available\nat https://github.com/neuralmind-ai/coliee and\nhttps://neuralsearchx.neuralmind.ai, respectively.",
    "url": "http://arxiv.org/abs/2205.15172v1"
  },
  {
    "reference": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
    "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
    "authors": [
      "Victor Sanh",
      "Albert Webson",
      "Colin Raffel",
      "Stephen H. Bach",
      "Lintang Sutawika",
      "Zaid Alyafeai",
      "Antoine Chaffin",
      "Arnaud Stiegler",
      "Teven Le Scao",
      "Arun Raja",
      "Manan Dey",
      "M Saiful Bari",
      "Canwen Xu",
      "Urmish Thakker",
      "Shanya Sharma Sharma",
      "Eliza Szczechla",
      "Taewoon Kim",
      "Gunjan Chhablani",
      "Nihal Nayak",
      "Debajyoti Datta",
      "Jonathan Chang",
      "Mike Tian-Jian Jiang",
      "Han Wang",
      "Matteo Manica",
      "Sheng Shen",
      "Zheng Xin Yong",
      "Harshit Pandey",
      "Rachel Bawden",
      "Thomas Wang",
      "Trishala Neeraj",
      "Jos Rozen",
      "Abheesht Sharma",
      "Andrea Santilli",
      "Thibault Fevry",
      "Jason Alan Fries",
      "Ryan Teehan",
      "Tali Bers",
      "Stella Biderman",
      "Leo Gao",
      "Thomas Wolf",
      "Alexander M. Rush"
    ],
    "year": 2021,
    "abstract": "Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\nhypothesized that this is a consequence of implicit multitask learning in\nlanguage models' pretraining (Radford et al., 2019). Can zero-shot\ngeneralization instead be directly induced by explicit multitask learning? To\ntest this question at scale, we develop a system for easily mapping any natural\nlanguage tasks into a human-readable prompted form. We convert a large set of\nsupervised datasets, each with multiple prompts with diverse wording. These\nprompted datasets allow for benchmarking the ability of a model to perform\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\nwide variety of tasks. The model attains strong zero-shot performance on\nseveral standard datasets, often outperforming models up to 16x its size.\nFurther, our approach attains strong performance on a subset of tasks from the\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\nare available at https://github.com/bigscience-workshop/promptsource.",
    "url": "http://arxiv.org/abs/2110.08207v3"
  },
  {
    "reference": "Bloom A 176b-parameter openaccess multilingual language model",
    "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
    "authors": [
      "BigScience Workshop",
      ":",
      "Teven Le Scao",
      "Angela Fan",
      "Christopher Akiki",
      "Ellie Pavlick",
      "Suzana Ili\u0107",
      "Daniel Hesslow",
      "Roman Castagn\u00e9",
      "Alexandra Sasha Luccioni",
      "Fran\u00e7ois Yvon",
      "Matthias Gall\u00e9",
      "Jonathan Tow",
      "Alexander M. Rush",
      "Stella Biderman",
      "Albert Webson",
      "Pawan Sasanka Ammanamanchi",
      "Thomas Wang",
      "Beno\u00eet Sagot",
      "Niklas Muennighoff",
      "Albert Villanova del Moral",
      "Olatunji Ruwase",
      "Rachel Bawden",
      "Stas Bekman",
      "Angelina McMillan-Major",
      "Iz Beltagy",
      "Huu Nguyen",
      "Lucile Saulnier",
      "Samson Tan",
      "Pedro Ortiz Suarez",
      "Victor Sanh",
      "Hugo Lauren\u00e7on",
      "Yacine Jernite",
      "Julien Launay",
      "Margaret Mitchell",
      "Colin Raffel",
      "Aaron Gokaslan",
      "Adi Simhi",
      "Aitor Soroa",
      "Alham Fikri Aji",
      "Amit Alfassy",
      "Anna Rogers",
      "Ariel Kreisberg Nitzav",
      "Canwen Xu",
      "Chenghao Mou",
      "Chris Emezue",
      "Christopher Klamm",
      "Colin Leong",
      "Daniel van Strien",
      "David Ifeoluwa Adelani",
      "Dragomir Radev",
      "Eduardo Gonz\u00e1lez Ponferrada",
      "Efrat Levkovizh",
      "Ethan Kim",
      "Eyal Bar Natan",
      "Francesco De Toni",
      "G\u00e9rard Dupont",
      "Germ\u00e1n Kruszewski",
      "Giada Pistilli",
      "Hady Elsahar",
      "Hamza Benyamina",
      "Hieu Tran",
      "Ian Yu",
      "Idris Abdulmumin",
      "Isaac Johnson",
      "Itziar Gonzalez-Dios",
      "Javier de la Rosa",
      "Jenny Chim",
      "Jesse Dodge",
      "Jian Zhu",
      "Jonathan Chang",
      "J\u00f6rg Frohberg",
      "Joseph Tobing",
      "Joydeep Bhattacharjee",
      "Khalid Almubarak",
      "Kimbo Chen",
      "Kyle Lo",
      "Leandro Von Werra",
      "Leon Weber",
      "Long Phan",
      "Loubna Ben allal",
      "Ludovic Tanguy",
      "Manan Dey",
      "Manuel Romero Mu\u00f1oz",
      "Maraim Masoud",
      "Mar\u00eda Grandury",
      "Mario \u0160a\u0161ko",
      "Max Huang",
      "Maximin Coavoux",
      "Mayank Singh",
      "Mike Tian-Jian Jiang",
      "Minh Chien Vu",
      "Mohammad A. Jauhar",
      "Mustafa Ghaleb",
      "Nishant Subramani",
      "Nora Kassner",
      "Nurulaqilla Khamis",
      "Olivier Nguyen",
      "Omar Espejel",
      "Ona de Gibert",
      "Paulo Villegas",
      "Peter Henderson",
      "Pierre Colombo",
      "Priscilla Amuok",
      "Quentin Lhoest",
      "Rheza Harliman",
      "Rishi Bommasani",
      "Roberto Luis L\u00f3pez",
      "Rui Ribeiro",
      "Salomey Osei",
      "Sampo Pyysalo",
      "Sebastian Nagel",
      "Shamik Bose",
      "Shamsuddeen Hassan Muhammad",
      "Shanya Sharma",
      "Shayne Longpre",
      "Somaieh Nikpoor",
      "Stanislav Silberberg",
      "Suhas Pai",
      "Sydney Zink",
      "Tiago Timponi Torrent",
      "Timo Schick",
      "Tristan Thrush",
      "Valentin Danchev",
      "Vassilina Nikoulina",
      "Veronika Laippala",
      "Violette Lepercq",
      "Vrinda Prabhu",
      "Zaid Alyafeai",
      "Zeerak Talat",
      "Arun Raja",
      "Benjamin Heinzerling",
      "Chenglei Si",
      "Davut Emre Ta\u015far",
      "Elizabeth Salesky",
      "Sabrina J. Mielke",
      "Wilson Y. Lee",
      "Abheesht Sharma",
      "Andrea Santilli",
      "Antoine Chaffin",
      "Arnaud Stiegler",
      "Debajyoti Datta",
      "Eliza Szczechla",
      "Gunjan Chhablani",
      "Han Wang",
      "Harshit Pandey",
      "Hendrik Strobelt",
      "Jason Alan Fries",
      "Jos Rozen",
      "Leo Gao",
      "Lintang Sutawika",
      "M Saiful Bari",
      "Maged S. Al-shaibani",
      "Matteo Manica",
      "Nihal Nayak",
      "Ryan Teehan",
      "Samuel Albanie",
      "Sheng Shen",
      "Srulik Ben-David",
      "Stephen H. Bach",
      "Taewoon Kim",
      "Tali Bers",
      "Thibault Fevry",
      "Trishala Neeraj",
      "Urmish Thakker",
      "Vikas Raunak",
      "Xiangru Tang",
      "Zheng-Xin Yong",
      "Zhiqing Sun",
      "Shaked Brody",
      "Yallow Uri",
      "Hadar Tojarieh",
      "Adam Roberts",
      "Hyung Won Chung",
      "Jaesung Tae",
      "Jason Phang",
      "Ofir Press",
      "Conglong Li",
      "Deepak Narayanan",
      "Hatim Bourfoune",
      "Jared Casper",
      "Jeff Rasley",
      "Max Ryabinin",
      "Mayank Mishra",
      "Minjia Zhang",
      "Mohammad Shoeybi",
      "Myriam Peyrounette",
      "Nicolas Patry",
      "Nouamane Tazi",
      "Omar Sanseviero",
      "Patrick von Platen",
      "Pierre Cornette",
      "Pierre Fran\u00e7ois Lavall\u00e9e",
      "R\u00e9mi Lacroix",
      "Samyam Rajbhandari",
      "Sanchit Gandhi",
      "Shaden Smith",
      "St\u00e9phane Requena",
      "Suraj Patil",
      "Tim Dettmers",
      "Ahmed Baruwa",
      "Amanpreet Singh",
      "Anastasia Cheveleva",
      "Anne-Laure Ligozat",
      "Arjun Subramonian",
      "Aur\u00e9lie N\u00e9v\u00e9ol",
      "Charles Lovering",
      "Dan Garrette",
      "Deepak Tunuguntla",
      "Ehud Reiter",
      "Ekaterina Taktasheva",
      "Ekaterina Voloshina",
      "Eli Bogdanov",
      "Genta Indra Winata",
      "Hailey Schoelkopf",
      "Jan-Christoph Kalo",
      "Jekaterina Novikova",
      "Jessica Zosa Forde",
      "Jordan Clive",
      "Jungo Kasai",
      "Ken Kawamura",
      "Liam Hazan",
      "Marine Carpuat",
      "Miruna Clinciu",
      "Najoung Kim",
      "Newton Cheng",
      "Oleg Serikov",
      "Omer Antverg",
      "Oskar van der Wal",
      "Rui Zhang",
      "Ruochen Zhang",
      "Sebastian Gehrmann",
      "Shachar Mirkin",
      "Shani Pais",
      "Tatiana Shavrina",
      "Thomas Scialom",
      "Tian Yun",
      "Tomasz Limisiewicz",
      "Verena Rieser",
      "Vitaly Protasov",
      "Vladislav Mikhailov",
      "Yada Pruksachatkun",
      "Yonatan Belinkov",
      "Zachary Bamberger",
      "Zden\u011bk Kasner",
      "Alice Rueda",
      "Amanda Pestana",
      "Amir Feizpour",
      "Ammar Khan",
      "Amy Faranak",
      "Ana Santos",
      "Anthony Hevia",
      "Antigona Unldreaj",
      "Arash Aghagol",
      "Arezoo Abdollahi",
      "Aycha Tammour",
      "Azadeh HajiHosseini",
      "Bahareh Behroozi",
      "Benjamin Ajibade",
      "Bharat Saxena",
      "Carlos Mu\u00f1oz Ferrandis",
      "Daniel McDuff",
      "Danish Contractor",
      "David Lansky",
      "Davis David",
      "Douwe Kiela",
      "Duong A. Nguyen",
      "Edward Tan",
      "Emi Baylor",
      "Ezinwanne Ozoani",
      "Fatima Mirza",
      "Frankline Ononiwu",
      "Habib Rezanejad",
      "Hessie Jones",
      "Indrani Bhattacharya",
      "Irene Solaiman",
      "Irina Sedenko",
      "Isar Nejadgholi",
      "Jesse Passmore",
      "Josh Seltzer",
      "Julio Bonis Sanz",
      "Livia Dutra",
      "Mairon Samagaio",
      "Maraim Elbadri",
      "Margot Mieskes",
      "Marissa Gerchick",
      "Martha Akinlolu",
      "Michael McKenna",
      "Mike Qiu",
      "Muhammed Ghauri",
      "Mykola Burynok",
      "Nafis Abrar",
      "Nazneen Rajani",
      "Nour Elkott",
      "Nour Fahmy",
      "Olanrewaju Samuel",
      "Ran An",
      "Rasmus Kromann",
      "Ryan Hao",
      "Samira Alizadeh",
      "Sarmad Shubber",
      "Silas Wang",
      "Sourav Roy",
      "Sylvain Viguier",
      "Thanh Le",
      "Tobi Oyebade",
      "Trieu Le",
      "Yoyo Yang",
      "Zach Nguyen",
      "Abhinav Ramesh Kashyap",
      "Alfredo Palasciano",
      "Alison Callahan",
      "Anima Shukla",
      "Antonio Miranda-Escalada",
      "Ayush Singh",
      "Benjamin Beilharz",
      "Bo Wang",
      "Caio Brito",
      "Chenxi Zhou",
      "Chirag Jain",
      "Chuxin Xu",
      "Cl\u00e9mentine Fourrier",
      "Daniel Le\u00f3n Peri\u00f1\u00e1n",
      "Daniel Molano",
      "Dian Yu",
      "Enrique Manjavacas",
      "Fabio Barth",
      "Florian Fuhrimann",
      "Gabriel Altay",
      "Giyaseddin Bayrak",
      "Gully Burns",
      "Helena U. Vrabec",
      "Imane Bello",
      "Ishani Dash",
      "Jihyun Kang",
      "John Giorgi",
      "Jonas Golde",
      "Jose David Posada",
      "Karthik Rangasai Sivaraman",
      "Lokesh Bulchandani",
      "Lu Liu",
      "Luisa Shinzato",
      "Madeleine Hahn de Bykhovetz",
      "Maiko Takeuchi",
      "Marc P\u00e0mies",
      "Maria A Castillo",
      "Marianna Nezhurina",
      "Mario S\u00e4nger",
      "Matthias Samwald",
      "Michael Cullan",
      "Michael Weinberg",
      "Michiel De Wolf",
      "Mina Mihaljcic",
      "Minna Liu",
      "Moritz Freidank",
      "Myungsun Kang",
      "Natasha Seelam",
      "Nathan Dahlberg",
      "Nicholas Michio Broad",
      "Nikolaus Muellner",
      "Pascale Fung",
      "Patrick Haller",
      "Ramya Chandrasekhar",
      "Renata Eisenberg",
      "Robert Martin",
      "Rodrigo Canalli",
      "Rosaline Su",
      "Ruisi Su",
      "Samuel Cahyawijaya",
      "Samuele Garda",
      "Shlok S Deshmukh",
      "Shubhanshu Mishra",
      "Sid Kiblawi",
      "Simon Ott",
      "Sinee Sang-aroonsiri",
      "Srishti Kumar",
      "Stefan Schweter",
      "Sushil Bharati",
      "Tanmay Laud",
      "Th\u00e9o Gigant",
      "Tomoya Kainuma",
      "Wojciech Kusa",
      "Yanis Labrak",
      "Yash Shailesh Bajaj",
      "Yash Venkatraman",
      "Yifan Xu",
      "Yingxin Xu",
      "Yu Xu",
      "Zhe Tan",
      "Zhongli Xie",
      "Zifan Ye",
      "Mathilde Bras",
      "Younes Belkada",
      "Thomas Wolf"
    ],
    "year": 2022,
    "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.",
    "url": "http://arxiv.org/abs/2211.05100v4"
  },
  {
    "reference": "Text Classification via Large Language Models",
    "title": "Text Classification via Large Language Models",
    "authors": [
      "Xiaofei Sun",
      "Xiaoya Li",
      "Jiwei Li",
      "Fei Wu",
      "Shangwei Guo",
      "Tianwei Zhang",
      "Guoyin Wang"
    ],
    "year": 2023,
    "abstract": "Despite the remarkable success of large-scale Language Models (LLMs) such as\nGPT-3, their performances still significantly underperform fine-tuned models in\nthe task of text classification. This is due to (1) the lack of reasoning\nability in addressing complex linguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of tokens allowed in in-context\nlearning.\n  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to addressing the complex linguistic\nphenomena involved in text classification: CARP first prompts LLMs to find\nsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),\nbased on which a diagnostic reasoning process is induced for final decisions.\nTo further address the limited-token issue, CARP uses a fine-tuned model on the\nsupervised dataset for $k$NN demonstration search in the in-context learning,\nallowing the model to take the advantage of both LLM's generalization ability\nand the task-specific evidence provided by the full labeled dataset.\nRemarkably, CARP yields new SOTA performances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on\nAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP\ndelivers impressive abilities on low-resource and domain-adaptation setups.\nSpecifically, using 16 examples per class, CARP achieves comparable\nperformances to supervised models with 1,024 examples per class.",
    "url": "http://arxiv.org/abs/2305.08377v3"
  },
  {
    "reference": "Attention Is All You Need",
    "title": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "year": 2017,
    "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
    "url": "http://arxiv.org/abs/1706.03762v7"
  },
  {
    "reference": "mT5 A Massively Multilingual Pretrained Text-to-Text Transformer",
    "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
    "authors": [
      "Linting Xue",
      "Noah Constant",
      "Adam Roberts",
      "Mihir Kale",
      "Rami Al-Rfou",
      "Aditya Siddhant",
      "Aditya Barua",
      "Colin Raffel"
    ],
    "year": 2020,
    "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified\ntext-to-text format and scale to attain state-of-the-art results on a wide\nvariety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based\ndataset covering 101 languages. We detail the design and modified training of\nmT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent \"accidental\ntranslation\" in the zero-shot setting, where a generative model chooses to\n(partially) translate its prediction into the wrong language. All of the code\nand model checkpoints used in this work are publicly available.",
    "url": "http://arxiv.org/abs/2010.11934v3"
  }
]