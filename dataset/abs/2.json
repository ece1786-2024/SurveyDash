[
  {
    "reference": "Deep contextualised  word representations",
    "title": "Deep contextualized word representations",
    "authors": [
      "Matthew E. Peters",
      "Mark Neumann",
      "Mohit Iyyer",
      "Matt Gardner",
      "Christopher Clark",
      "Kenton Lee",
      "Luke Zettlemoyer"
    ],
    "year": 2018,
    "abstract": "We introduce a new type of deep contextualized word representation that\nmodels both (1) complex characteristics of word use (e.g., syntax and\nsemantics), and (2) how these uses vary across linguistic contexts (i.e., to\nmodel polysemy). Our word vectors are learned functions of the internal states\nof a deep bidirectional language model (biLM), which is pre-trained on a large\ntext corpus. We show that these representations can be easily added to existing\nmodels and significantly improve the state of the art across six challenging\nNLP problems, including question answering, textual entailment and sentiment\nanalysis. We also present an analysis showing that exposing the deep internals\nof the pre-trained network is crucial, allowing downstream models to mix\ndifferent types of semi-supervision signals.",
    "url": "http://arxiv.org/abs/1802.05365v2"
  },
  {
    "reference": "BERT Pre-training of  Deep Bidirectional Transformers for Language Understanding",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "year": 2018,
    "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
    "url": "http://arxiv.org/abs/1810.04805v2"
  },
  {
    "reference": "Deep Transfer Learning  Beyond  Transformer Language Models in Information Systems Research",
    "title": "Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research",
    "authors": [
      "Ross Gruetzemacher",
      "David Paradice"
    ],
    "year": 2021,
    "abstract": "AI is widely thought to be poised to transform business, yet current\nperceptions of the scope of this transformation may be myopic. Recent progress\nin natural language processing involving transformer language models (TLMs)\noffers a potential avenue for AI-driven business and societal transformation\nthat is beyond the scope of what most currently foresee. We review this recent\nprogress as well as recent literature utilizing text mining in top IS journals\nto develop an outline for how future IS research can benefit from these new\ntechniques. Our review of existing IS literature reveals that suboptimal text\nmining techniques are prevalent and that the more advanced TLMs could be\napplied to enhance and increase IS research involving text data, and to enable\nnew IS research topics, thus creating more value for the research community.\nThis is possible because these techniques make it easier to develop very\npowerful custom systems and their performance is superior to existing methods\nfor a wide range of tasks and applications. Further, multilingual language\nmodels make possible higher quality text analytics for research in multiple\nlanguages. We also identify new avenues for IS research, like language user\ninterfaces, that may offer even greater potential for future IS research.",
    "url": "http://arxiv.org/abs/2110.08975v2"
  },
  {
    "reference": "LLaMA Open and Efficient Foundation Language  Models",
    "title": "LLaMA: Open and Efficient Foundation Language Models",
    "authors": [
      "Hugo Touvron",
      "Thibaut Lavril",
      "Gautier Izacard",
      "Xavier Martinet",
      "Marie-Anne Lachaux",
      "Timoth\u00e9e Lacroix",
      "Baptiste Rozi\u00e8re",
      "Naman Goyal",
      "Eric Hambro",
      "Faisal Azhar",
      "Aurelien Rodriguez",
      "Armand Joulin",
      "Edouard Grave",
      "Guillaume Lample"
    ],
    "year": 2023,
    "abstract": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.",
    "url": "http://arxiv.org/abs/2302.13971v1"
  },
  {
    "reference": "Root Mean Square Layer Normalisation",
    "title": "Root Mean Square Layer Normalization",
    "authors": [
      "Biao Zhang",
      "Rico Sennrich"
    ],
    "year": 2019,
    "abstract": "Layer normalization (LayerNorm) has been successfully applied to various deep\nneural networks to help stabilize training and boost model convergence because\nof its capability in handling re-centering and re-scaling of both inputs and\nweight matrix. However, the computational overhead introduced by LayerNorm\nmakes these improvements expensive and significantly slows the underlying\nnetwork, e.g. RNN in particular. In this paper, we hypothesize that\nre-centering invariance in LayerNorm is dispensable and propose root mean\nsquare layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs\nto a neuron in one layer according to root mean square (RMS), giving the model\nre-scaling invariance property and implicit learning rate adaptation ability.\nRMSNorm is computationally simpler and thus more efficient than LayerNorm. We\nalso present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of\nthe summed inputs without breaking the above properties. Extensive experiments\non several tasks using diverse network architectures show that RMSNorm achieves\ncomparable performance against LayerNorm but reduces the running time by 7%~64%\non different models. Source code is available at\nhttps://github.com/bzhangGo/rmsnorm.",
    "url": "http://arxiv.org/abs/1910.07467v1"
  },
  {
    "reference": "Exploring the Limits of Transfer Learning with a  Unified Text-to-Text Transformer",
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "authors": [
      "Colin Raffel",
      "Noam Shazeer",
      "Adam Roberts",
      "Katherine Lee",
      "Sharan Narang",
      "Michael Matena",
      "Yanqi Zhou",
      "Wei Li",
      "Peter J. Liu"
    ],
    "year": 2019,
    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.",
    "url": "http://arxiv.org/abs/1910.10683v4"
  },
  {
    "reference": "Reformer Enhanced Transformer With Rotary Position  Embedding",
    "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "authors": [
      "Jianlin Su",
      "Yu Lu",
      "Shengfeng Pan",
      "Ahmed Murtadha",
      "Bo Wen",
      "Yunfeng Liu"
    ],
    "year": 2021,
    "abstract": "Position encoding recently has shown effective in the transformer\narchitecture. It enables valuable supervision for dependency modeling between\nelements at different positions of the sequence. In this paper, we first\ninvestigate various methods to integrate positional information into the\nlearning process of transformer-based language models. Then, we propose a novel\nmethod named Rotary Position Embedding(RoPE) to effectively leverage the\npositional information. Specifically, the proposed RoPE encodes the absolute\nposition with a rotation matrix and meanwhile incorporates the explicit\nrelative position dependency in self-attention formulation. Notably, RoPE\nenables valuable properties, including the flexibility of sequence length,\ndecaying inter-token dependency with increasing relative distances, and the\ncapability of equipping the linear self-attention with relative position\nencoding. Finally, we evaluate the enhanced transformer with rotary position\nembedding, also called RoFormer, on various long text classification benchmark\ndatasets. Our experiments show that it consistently overcomes its alternatives.\nFurthermore, we provide a theoretical analysis to explain some experimental\nresults. RoFormer is already integrated into Huggingface:\n\\url{https://huggingface.co/docs/transformers/model_doc/roformer}.",
    "url": "http://arxiv.org/abs/2104.09864v5"
  },
  {
    "reference": "Decoupled Weight Decay Regularization",
    "title": "Decoupled Weight Decay Regularization",
    "authors": [
      "Ilya Loshchilov",
      "Frank Hutter"
    ],
    "year": 2017,
    "abstract": "L$_2$ regularization and weight decay regularization are equivalent for\nstandard stochastic gradient descent (when rescaled by the learning rate), but\nas we demonstrate this is \\emph{not} the case for adaptive gradient algorithms,\nsuch as Adam. While common implementations of these algorithms employ L$_2$\nregularization (often calling it \"weight decay\" in what may be misleading due\nto the inequivalence we expose), we propose a simple modification to recover\nthe original formulation of weight decay regularization by \\emph{decoupling}\nthe weight decay from the optimization steps taken w.r.t. the loss function. We\nprovide empirical evidence that our proposed modification (i) decouples the\noptimal choice of weight decay factor from the setting of the learning rate for\nboth standard SGD and Adam and (ii) substantially improves Adam's\ngeneralization performance, allowing it to compete with SGD with momentum on\nimage classification datasets (on which it was previously typically\noutperformed by the latter). Our proposed decoupled weight decay has already\nbeen adopted by many researchers, and the community has implemented it in\nTensorFlow and PyTorch; the complete source code for our experiments is\navailable at https://github.com/loshchil/AdamW-and-SGDW",
    "url": "http://arxiv.org/abs/1711.05101v3"
  }
]