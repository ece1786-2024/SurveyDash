[
  {
    "reference": "Large language models in education Vision and opportunities",
    "title": "Large Language Models in Education: Vision and Opportunities",
    "authors": [
      "Wensheng Gan",
      "Zhenlian Qi",
      "Jiayang Wu",
      "Jerry Chun-Wei Lin"
    ],
    "year": 2023,
    "abstract": "With the rapid development of artificial intelligence technology, large\nlanguage models (LLMs) have become a hot research topic. Education plays an\nimportant role in human social development and progress. Traditional education\nfaces challenges such as individual student differences, insufficient\nallocation of teaching resources, and assessment of teaching effectiveness.\nTherefore, the applications of LLMs in the field of digital/smart education\nhave broad prospects. The research on educational large models (EduLLMs) is\nconstantly evolving, providing new methods and approaches to achieve\npersonalized learning, intelligent tutoring, and educational assessment goals,\nthereby improving the quality of education and the learning experience. This\narticle aims to investigate and summarize the application of LLMs in smart\neducation. It first introduces the research background and motivation of LLMs\nand explains the essence of LLMs. It then discusses the relationship between\ndigital education and EduLLMs and summarizes the current research status of\neducational large models. The main contributions are the systematic summary and\nvision of the research background, motivation, and application of large models\nfor education (LLM4Edu). By reviewing existing research, this article provides\nguidance and insights for educators, researchers, and policy-makers to gain a\ndeep understanding of the potential and challenges of LLM4Edu. It further\nprovides guidance for further advancing the development and application of\nLLM4Edu, while still facing technical, ethical, and practical challenges\nrequiring further research and exploration.",
    "url": "http://arxiv.org/abs/2311.13160v1"
  },
  {
    "reference": "Model-as-a-service MaaS A survey",
    "title": "Model-as-a-Service (MaaS): A Survey",
    "authors": [
      "Wensheng Gan",
      "Shicheng Wan",
      "Philip S. Yu"
    ],
    "year": 2023,
    "abstract": "Due to the increased number of parameters and data in the pre-trained model\nexceeding a certain level, a foundation model (e.g., a large language model)\ncan significantly improve downstream task performance and emerge with some\nnovel special abilities (e.g., deep learning, complex reasoning, and human\nalignment) that were not present before. Foundation models are a form of\ngenerative artificial intelligence (GenAI), and Model-as-a-Service (MaaS) has\nemerged as a groundbreaking paradigm that revolutionizes the deployment and\nutilization of GenAI models. MaaS represents a paradigm shift in how we use AI\ntechnologies and provides a scalable and accessible solution for developers and\nusers to leverage pre-trained AI models without the need for extensive\ninfrastructure or expertise in model training. In this paper, the introduction\naims to provide a comprehensive overview of MaaS, its significance, and its\nimplications for various industries. We provide a brief review of the\ndevelopment history of \"X-as-a-Service\" based on cloud computing and present\nthe key technologies involved in MaaS. The development of GenAI models will\nbecome more democratized and flourish. We also review recent application\nstudies of MaaS. Finally, we highlight several challenges and future issues in\nthis promising area. MaaS is a new deployment and service paradigm for\ndifferent AI-based models. We hope this review will inspire future research in\nthe field of MaaS.",
    "url": "http://arxiv.org/abs/2311.05804v1"
  },
  {
    "reference": "BERT Pretraining of deep bidirectional transformers for language understanding",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "year": 2018,
    "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
    "url": "http://arxiv.org/abs/1810.04805v2"
  },
  {
    "reference": "RoBERTa A robustly optimized bert pretraining approach",
    "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    "authors": [
      "Yinhan Liu",
      "Myle Ott",
      "Naman Goyal",
      "Jingfei Du",
      "Mandar Joshi",
      "Danqi Chen",
      "Omer Levy",
      "Mike Lewis",
      "Luke Zettlemoyer",
      "Veselin Stoyanov"
    ],
    "year": 2019,
    "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
    "url": "http://arxiv.org/abs/1907.11692v1"
  },
  {
    "reference": "A review on methods and applications in multimodal deep learning",
    "title": "A Review on Methods and Applications in Multimodal Deep Learning",
    "authors": [
      "Jabeen Summaira",
      "Xi Li",
      "Amin Muhammad Shoib",
      "Jabbar Abdul"
    ],
    "year": 2022,
    "abstract": "Deep Learning has implemented a wide range of applications and has become\nincreasingly popular in recent years. The goal of multimodal deep learning\n(MMDL) is to create models that can process and link information using various\nmodalities. Despite the extensive development made for unimodal learning, it\nstill cannot cover all the aspects of human learning. Multimodal learning helps\nto understand and analyze better when various senses are engaged in the\nprocessing of information. This paper focuses on multiple types of modalities,\ni.e., image, video, text, audio, body gestures, facial expressions, and\nphysiological signals. Detailed analysis of the baseline approaches and an\nin-depth study of recent advancements during the last five years (2017 to 2021)\nin multimodal deep learning applications has been provided. A fine-grained\ntaxonomy of various multimodal deep learning methods is proposed, elaborating\non different applications in more depth. Lastly, main issues are highlighted\nseparately for each domain, along with their possible future research\ndirections.",
    "url": "http://arxiv.org/abs/2202.09195v1"
  },
  {
    "reference": "A survey on multimodal large language models",
    "title": "A Survey on Multimodal Large Language Models",
    "authors": [
      "Shukang Yin",
      "Chaoyou Fu",
      "Sirui Zhao",
      "Ke Li",
      "Xing Sun",
      "Tong Xu",
      "Enhong Chen"
    ],
    "year": 2023,
    "abstract": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.",
    "url": "http://arxiv.org/abs/2306.13549v4"
  },
  {
    "reference": "Multimodal deep learning",
    "title": "Multimodal Deep Learning",
    "authors": [
      "Cem Akkus",
      "Luyang Chu",
      "Vladana Djakovic",
      "Steffen Jauch-Walser",
      "Philipp Koch",
      "Giacomo Loss",
      "Christopher Marquardt",
      "Marco Moldovan",
      "Nadja Sauter",
      "Maximilian Schneider",
      "Rickmer Schulte",
      "Karol Urbanczyk",
      "Jann Goschenhofer",
      "Christian Heumann",
      "Rasmus Hvingelby",
      "Daniel Schalk",
      "Matthias A\u00dfenmacher"
    ],
    "year": 2023,
    "abstract": "This book is the result of a seminar in which we reviewed multimodal\napproaches and attempted to create a solid overview of the field, starting with\nthe current state-of-the-art approaches in the two subfields of Deep Learning\nindividually. Further, modeling frameworks are discussed where one modality is\ntransformed into the other, as well as models in which one modality is utilized\nto enhance representation learning for the other. To conclude the second part,\narchitectures with a focus on handling both modalities simultaneously are\nintroduced. Finally, we also cover other modalities as well as general-purpose\nmulti-modal models, which are able to handle different tasks on different\nmodalities within one unified architecture. One interesting application\n(Generative Art) eventually caps off this booklet.",
    "url": "http://arxiv.org/abs/2301.04856v1"
  },
  {
    "reference": "Learning transferable visual models from natural language supervision",
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "authors": [
      "Alec Radford",
      "Jong Wook Kim",
      "Chris Hallacy",
      "Aditya Ramesh",
      "Gabriel Goh",
      "Sandhini Agarwal",
      "Girish Sastry",
      "Amanda Askell",
      "Pamela Mishkin",
      "Jack Clark",
      "Gretchen Krueger",
      "Ilya Sutskever"
    ],
    "year": 2021,
    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set\nof predetermined object categories. This restricted form of supervision limits\ntheir generality and usability since additional labeled data is needed to\nspecify any other visual concept. Learning directly from raw text about images\nis a promising alternative which leverages a much broader source of\nsupervision. We demonstrate that the simple pre-training task of predicting\nwhich caption goes with which image is an efficient and scalable way to learn\nSOTA image representations from scratch on a dataset of 400 million (image,\ntext) pairs collected from the internet. After pre-training, natural language\nis used to reference learned visual concepts (or describe new ones) enabling\nzero-shot transfer of the model to downstream tasks. We study the performance\nof this approach by benchmarking on over 30 different existing computer vision\ndatasets, spanning tasks such as OCR, action recognition in videos,\ngeo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a\nfully supervised baseline without the need for any dataset specific training.\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28 million training examples it\nwas trained on. We release our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP.",
    "url": "http://arxiv.org/abs/2103.00020v1"
  },
  {
    "reference": "Hierarchical text-conditional image generation with clip latents",
    "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
    "authors": [
      "Aditya Ramesh",
      "Prafulla Dhariwal",
      "Alex Nichol",
      "Casey Chu",
      "Mark Chen"
    ],
    "year": 2022,
    "abstract": "Contrastive models like CLIP have been shown to learn robust representations\nof images that capture both semantics and style. To leverage these\nrepresentations for image generation, we propose a two-stage model: a prior\nthat generates a CLIP image embedding given a text caption, and a decoder that\ngenerates an image conditioned on the image embedding. We show that explicitly\ngenerating image representations improves image diversity with minimal loss in\nphotorealism and caption similarity. Our decoders conditioned on image\nrepresentations can also produce variations of an image that preserve both its\nsemantics and style, while varying the non-essential details absent from the\nimage representation. Moreover, the joint embedding space of CLIP enables\nlanguage-guided image manipulations in a zero-shot fashion. We use diffusion\nmodels for the decoder and experiment with both autoregressive and diffusion\nmodels for the prior, finding that the latter are computationally more\nefficient and produce higher-quality samples.",
    "url": "http://arxiv.org/abs/2204.06125v1"
  },
  {
    "reference": "BEiT BERT pre-training of image transformers",
    "title": "BEiT: BERT Pre-Training of Image Transformers",
    "authors": [
      "Hangbo Bao",
      "Li Dong",
      "Songhao Piao",
      "Furu Wei"
    ],
    "year": 2021,
    "abstract": "We introduce a self-supervised vision representation model BEiT, which stands\nfor Bidirectional Encoder representation from Image Transformers. Following\nBERT developed in the natural language processing area, we propose a masked\nimage modeling task to pretrain vision Transformers. Specifically, each image\nhas two views in our pre-training, i.e, image patches (such as 16x16 pixels),\nand visual tokens (i.e., discrete tokens). We first \"tokenize\" the original\nimage into visual tokens. Then we randomly mask some image patches and fed them\ninto the backbone Transformer. The pre-training objective is to recover the\noriginal visual tokens based on the corrupted image patches. After pre-training\nBEiT, we directly fine-tune the model parameters on downstream tasks by\nappending task layers upon the pretrained encoder. Experimental results on\nimage classification and semantic segmentation show that our model achieves\ncompetitive results with previous pre-training methods. For example, base-size\nBEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming\nfrom-scratch DeiT training (81.8%) with the same setup. Moreover, large-size\nBEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with\nsupervised pre-training on ImageNet-22K (85.2%). The code and pretrained models\nare available at https://aka.ms/beit.",
    "url": "http://arxiv.org/abs/2106.08254v2"
  },
  {
    "reference": "Language is not all you need Aligning perception with language models",
    "title": "Language Is Not All You Need: Aligning Perception with Language Models",
    "authors": [
      "Shaohan Huang",
      "Li Dong",
      "Wenhui Wang",
      "Yaru Hao",
      "Saksham Singhal",
      "Shuming Ma",
      "Tengchao Lv",
      "Lei Cui",
      "Owais Khan Mohammed",
      "Barun Patra",
      "Qiang Liu",
      "Kriti Aggarwal",
      "Zewen Chi",
      "Johan Bjorck",
      "Vishrav Chaudhary",
      "Subhojit Som",
      "Xia Song",
      "Furu Wei"
    ],
    "year": 2023,
    "abstract": "A big convergence of language, multimodal perception, action, and world\nmodeling is a key step toward artificial general intelligence. In this work, we\nintroduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive\ngeneral modalities, learn in context (i.e., few-shot), and follow instructions\n(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale\nmultimodal corpora, including arbitrarily interleaved text and images,\nimage-caption pairs, and text data. We evaluate various settings, including\nzero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range\nof tasks without any gradient updates or finetuning. Experimental results show\nthat Kosmos-1 achieves impressive performance on (i) language understanding,\ngeneration, and even OCR-free NLP (directly fed with document images), (ii)\nperception-language tasks, including multimodal dialogue, image captioning,\nvisual question answering, and (iii) vision tasks, such as image recognition\nwith descriptions (specifying classification via text instructions). We also\nshow that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge\nfrom language to multimodal, and from multimodal to language. In addition, we\nintroduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning\ncapability of MLLMs.",
    "url": "http://arxiv.org/abs/2302.14045v2"
  },
  {
    "reference": "Palm-e An embodied multimodal language model",
    "title": "PaLM-E: An Embodied Multimodal Language Model",
    "authors": [
      "Danny Driess",
      "Fei Xia",
      "Mehdi S. M. Sajjadi",
      "Corey Lynch",
      "Aakanksha Chowdhery",
      "Brian Ichter",
      "Ayzaan Wahid",
      "Jonathan Tompson",
      "Quan Vuong",
      "Tianhe Yu",
      "Wenlong Huang",
      "Yevgen Chebotar",
      "Pierre Sermanet",
      "Daniel Duckworth",
      "Sergey Levine",
      "Vincent Vanhoucke",
      "Karol Hausman",
      "Marc Toussaint",
      "Klaus Greff",
      "Andy Zeng",
      "Igor Mordatch",
      "Pete Florence"
    ],
    "year": 2023,
    "abstract": "Large language models excel at a wide range of complex tasks. However,\nenabling general inference in the real world, e.g., for robotics problems,\nraises the challenge of grounding. We propose embodied language models to\ndirectly incorporate real-world continuous sensor modalities into language\nmodels and thereby establish the link between words and percepts. Input to our\nembodied language model are multi-modal sentences that interleave visual,\ncontinuous state estimation, and textual input encodings. We train these\nencodings end-to-end, in conjunction with a pre-trained large language model,\nfor multiple embodied tasks including sequential robotic manipulation planning,\nvisual question answering, and captioning. Our evaluations show that PaLM-E, a\nsingle large embodied multimodal model, can address a variety of embodied\nreasoning tasks, from a variety of observation modalities, on multiple\nembodiments, and further, exhibits positive transfer: the model benefits from\ndiverse joint training across internet-scale language, vision, and\nvisual-language domains. Our largest model, PaLM-E-562B with 562B parameters,\nin addition to being trained on robotics tasks, is a visual-language generalist\nwith state-of-the-art performance on OK-VQA, and retains generalist language\ncapabilities with increasing scale.",
    "url": "http://arxiv.org/abs/2303.03378v1"
  },
  {
    "reference": "Efficient estimation of word representations in vector space",
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "authors": [
      "Tomas Mikolov",
      "Kai Chen",
      "Greg Corrado",
      "Jeffrey Dean"
    ],
    "year": 2013,
    "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
    "url": "http://arxiv.org/abs/1301.3781v3"
  },
  {
    "reference": "Byte pair encoding is suboptimal for language model pretraining",
    "title": "Byte Pair Encoding is Suboptimal for Language Model Pretraining",
    "authors": [
      "Kaj Bostrom",
      "Greg Durrett"
    ],
    "year": 2020,
    "abstract": "The success of pretrained transformer language models (LMs) in natural\nlanguage processing has led to a wide range of pretraining setups. In\nparticular, these models employ a variety of subword tokenization methods, most\nnotably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the\nWordPiece method (Schuster and Nakajima, 2012), and unigram language modeling\n(Kudo, 2018), to segment text. However, to the best of our knowledge, the\nliterature does not contain a direct evaluation of the impact of tokenization\non language model pretraining. We analyze differences between BPE and unigram\nLM tokenization, finding that the latter method recovers subword units that\nalign more closely with morphology and avoids problems stemming from BPE's\ngreedy construction procedure. We then compare the fine-tuned task performance\nof identical transformer masked language models pretrained with these\ntokenizations. Across downstream tasks and two languages (English and\nJapanese), we find that the unigram LM tokenization method matches or\noutperforms BPE. We hope that developers of future pretrained LMs will consider\nadopting the unigram LM method over the more prevalent BPE.",
    "url": "http://arxiv.org/abs/2004.03720v2"
  },
  {
    "reference": "Visual concepts tokenization",
    "title": "Visual Concepts Tokenization",
    "authors": [
      "Tao Yang",
      "Yuwang Wang",
      "Yan Lu",
      "Nanning Zheng"
    ],
    "year": 2022,
    "abstract": "Obtaining the human-like perception ability of abstracting visual concepts\nfrom concrete pixels has always been a fundamental and important target in\nmachine learning research fields such as disentangled representation learning\nand scene decomposition. Towards this goal, we propose an unsupervised\ntransformer-based Visual Concepts Tokenization framework, dubbed VCT, to\nperceive an image into a set of disentangled visual concept tokens, with each\nconcept token responding to one type of independent visual concept.\nParticularly, to obtain these concept tokens, we only use cross-attention to\nextract visual information from the image tokens layer by layer without\nself-attention between concept tokens, preventing information leakage across\nconcept tokens. We further propose a Concept Disentangling Loss to facilitate\nthat different concept tokens represent independent visual concepts. The\ncross-attention and disentangling loss play the role of induction and mutual\nexclusion for the concept tokens, respectively. Extensive experiments on\nseveral popular datasets verify the effectiveness of VCT on the tasks of\ndisentangled representation learning and scene decomposition. VCT achieves the\nstate of the art results by a large margin.",
    "url": "http://arxiv.org/abs/2205.10093v2"
  },
  {
    "reference": "An empirical study of training endto-end vision-and-language transformers",
    "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers",
    "authors": [
      "Zi-Yi Dou",
      "Yichong Xu",
      "Zhe Gan",
      "Jianfeng Wang",
      "Shuohang Wang",
      "Lijuan Wang",
      "Chenguang Zhu",
      "Pengchuan Zhang",
      "Lu Yuan",
      "Nanyun Peng",
      "Zicheng Liu",
      "Michael Zeng"
    ],
    "year": 2021,
    "abstract": "Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.",
    "url": "http://arxiv.org/abs/2111.02387v3"
  },
  {
    "reference": "Retrieval-based knowledge augmented vision language pre-training",
    "title": "Retrieval-based Knowledge Augmented Vision Language Pre-training",
    "authors": [
      "Jiahua Rao",
      "Zifei Shan",
      "Longpo Liu",
      "Yao Zhou",
      "Yuedong Yang"
    ],
    "year": 2023,
    "abstract": "With the recent progress in large-scale vision and language representation\nlearning, Vision Language Pre-training (VLP) models have achieved promising\nimprovements on various multi-modal downstream tasks. Albeit powerful, these\nmodels have not fully leveraged world knowledge to their advantage. A key\nchallenge of knowledge-augmented VLP is the lack of clear connections between\nknowledge and multi-modal data. Moreover, not all knowledge present in\nimages/texts is useful, therefore prior approaches often struggle to\neffectively integrate knowledge, visual, and textual information. In this\nstudy, we propose REtrieval-based knowledge Augmented Vision Language (REAVL),\na novel knowledge-augmented pre-training framework to address the above issues.\nFor the first time, we introduce a knowledge-aware self-supervised learning\nscheme that efficiently establishes the correspondence between knowledge and\nmulti-modal data and identifies informative knowledge to improve the modeling\nof alignment and interactions between visual and textual modalities. By\nadaptively integrating informative knowledge with visual and textual\ninformation, REAVL achieves new state-of-the-art performance uniformly on\nknowledge-based vision-language understanding and multi-modal entity linking\ntasks, as well as competitive results on general vision-language tasks while\nonly using 0.2% pre-training data of the best models. Our model shows strong\nsample efficiency and effective knowledge utilization.",
    "url": "http://arxiv.org/abs/2304.13923v2"
  },
  {
    "reference": "Align before fuse Vision and language representation learning with momentum distillation",
    "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
    "authors": [
      "Junnan Li",
      "Ramprasaath R. Selvaraju",
      "Akhilesh Deepak Gotmare",
      "Shafiq Joty",
      "Caiming Xiong",
      "Steven Hoi"
    ],
    "year": 2021,
    "abstract": "Large-scale vision and language representation learning has shown promising\nimprovements on various vision-language tasks. Most existing methods employ a\ntransformer-based multimodal encoder to jointly model visual tokens\n(region-based image features) and word tokens. Because the visual tokens and\nword tokens are unaligned, it is challenging for the multimodal encoder to\nlearn image-text interactions. In this paper, we introduce a contrastive loss\nto ALign the image and text representations BEfore Fusing (ALBEF) them through\ncross-modal attention, which enables more grounded vision and language\nrepresentation learning. Unlike most existing methods, our method does not\nrequire bounding box annotations nor high-resolution images. In order to\nimprove learning from noisy web data, we propose momentum distillation, a\nself-training method which learns from pseudo-targets produced by a momentum\nmodel. We provide a theoretical analysis of ALBEF from a mutual information\nmaximization perspective, showing that different training tasks can be\ninterpreted as different ways to generate views for an image-text pair. ALBEF\nachieves state-of-the-art performance on multiple downstream vision-language\ntasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained\non orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves\nabsolute improvements of 2.37% and 3.84% compared to the state-of-the-art,\nwhile enjoying faster inference speed. Code and pre-trained models are\navailable at https://github.com/salesforce/ALBEF/.",
    "url": "http://arxiv.org/abs/2107.07651v2"
  },
  {
    "reference": "Exploring the limits of transfer learning with a unified text-to-text transformer",
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "authors": [
      "Colin Raffel",
      "Noam Shazeer",
      "Adam Roberts",
      "Katherine Lee",
      "Sharan Narang",
      "Michael Matena",
      "Yanqi Zhou",
      "Wei Li",
      "Peter J. Liu"
    ],
    "year": 2019,
    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.",
    "url": "http://arxiv.org/abs/1910.10683v4"
  },
  {
    "reference": "SimVLM Simple visual language model pretraining with weak supervision",
    "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
    "authors": [
      "Zirui Wang",
      "Jiahui Yu",
      "Adams Wei Yu",
      "Zihang Dai",
      "Yulia Tsvetkov",
      "Yuan Cao"
    ],
    "year": 2021,
    "abstract": "With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.",
    "url": "http://arxiv.org/abs/2108.10904v3"
  },
  {
    "reference": "Vlmo Unified vision-language pre-training with mixture-of-modality-experts",
    "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
    "authors": [
      "Hangbo Bao",
      "Wenhui Wang",
      "Li Dong",
      "Qiang Liu",
      "Owais Khan Mohammed",
      "Kriti Aggarwal",
      "Subhojit Som",
      "Furu Wei"
    ],
    "year": 2021,
    "abstract": "We present a unified Vision-Language pretrained Model (VLMo) that jointly\nlearns a dual encoder and a fusion encoder with a modular Transformer network.\nSpecifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,\nwhere each block contains a pool of modality-specific experts and a shared\nself-attention layer. Because of the modeling flexibility of MoME, pretrained\nVLMo can be fine-tuned as a fusion encoder for vision-language classification\ntasks, or used as a dual encoder for efficient image-text retrieval. Moreover,\nwe propose a stagewise pre-training strategy, which effectively leverages\nlarge-scale image-only and text-only data besides image-text pairs.\nExperimental results show that VLMo achieves state-of-the-art results on\nvarious vision-language tasks, including VQA, NLVR2 and image-text retrieval.\nThe code and pretrained models are available at https://aka.ms/vlmo.",
    "url": "http://arxiv.org/abs/2111.02358v2"
  },
  {
    "reference": "A prompt pattern catalog to enhance prompt engineering with ChatGPT",
    "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
    "authors": [
      "Jules White",
      "Quchen Fu",
      "Sam Hays",
      "Michael Sandborn",
      "Carlos Olea",
      "Henry Gilbert",
      "Ashraf Elnashar",
      "Jesse Spencer-Smith",
      "Douglas C. Schmidt"
    ],
    "year": 2023,
    "abstract": "Prompt engineering is an increasingly important skill set needed to converse\neffectively with large language models (LLMs), such as ChatGPT. Prompts are\ninstructions given to an LLM to enforce rules, automate processes, and ensure\nspecific qualities (and quantities) of generated output. Prompts are also a\nform of programming that can customize the outputs and interactions with an\nLLM. This paper describes a catalog of prompt engineering techniques presented\nin pattern form that have been applied to solve common problems when conversing\nwith LLMs. Prompt patterns are a knowledge transfer method analogous to\nsoftware patterns since they provide reusable solutions to common problems\nfaced in a particular context, i.e., output generation and interaction when\nworking with LLMs. This paper provides the following contributions to research\non prompt engineering that apply LLMs to automate software development tasks.\nFirst, it provides a framework for documenting patterns for structuring prompts\nto solve a range of problems so that they can be adapted to different domains.\nSecond, it presents a catalog of patterns that have been applied successfully\nto improve the outputs of LLM conversations. Third, it explains how prompts can\nbe built from multiple patterns and illustrates prompt patterns that benefit\nfrom combination with other prompt patterns.",
    "url": "http://arxiv.org/abs/2302.11382v1"
  },
  {
    "reference": "Visual ChatGPT Talking drawing and editing with visual foundation models",
    "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
    "authors": [
      "Chenfei Wu",
      "Shengming Yin",
      "Weizhen Qi",
      "Xiaodong Wang",
      "Zecheng Tang",
      "Nan Duan"
    ],
    "year": 2023,
    "abstract": "ChatGPT is attracting a cross-field interest as it provides a language\ninterface with remarkable conversational competency and reasoning capabilities\nacross many domains. However, since ChatGPT is trained with languages, it is\ncurrently not capable of processing or generating images from the visual world.\nAt the same time, Visual Foundation Models, such as Visual Transformers or\nStable Diffusion, although showing great visual understanding and generation\ncapabilities, they are only experts on specific tasks with one-round fixed\ninputs and outputs. To this end, We build a system called \\textbf{Visual\nChatGPT}, incorporating different Visual Foundation Models, to enable the user\nto interact with ChatGPT by 1) sending and receiving not only languages but\nalso images 2) providing complex visual questions or visual editing\ninstructions that require the collaboration of multiple AI models with\nmulti-steps. 3) providing feedback and asking for corrected results. We design\na series of prompts to inject the visual model information into ChatGPT,\nconsidering models of multiple inputs/outputs and models that require visual\nfeedback. Experiments show that Visual ChatGPT opens the door to investigating\nthe visual roles of ChatGPT with the help of Visual Foundation Models. Our\nsystem is publicly available at\n\\url{https://github.com/microsoft/visual-chatgpt}.",
    "url": "http://arxiv.org/abs/2303.04671v1"
  },
  {
    "reference": "Attention is all you need",
    "title": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "year": 2017,
    "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
    "url": "http://arxiv.org/abs/1706.03762v7"
  },
  {
    "reference": "An image is worth 16x16 words Transformers for image recognition at scale",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "authors": [
      "Alexey Dosovitskiy",
      "Lucas Beyer",
      "Alexander Kolesnikov",
      "Dirk Weissenborn",
      "Xiaohua Zhai",
      "Thomas Unterthiner",
      "Mostafa Dehghani",
      "Matthias Minderer",
      "Georg Heigold",
      "Sylvain Gelly",
      "Jakob Uszkoreit",
      "Neil Houlsby"
    ],
    "year": 2020,
    "abstract": "While the Transformer architecture has become the de-facto standard for\nnatural language processing tasks, its applications to computer vision remain\nlimited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional\nnetworks while keeping their overall structure in place. We show that this\nreliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\nTransformer (ViT) attains excellent results compared to state-of-the-art\nconvolutional networks while requiring substantially fewer computational\nresources to train.",
    "url": "http://arxiv.org/abs/2010.11929v2"
  },
  {
    "reference": "MM-REACT Prompting chatgpt for multimodal reasoning and action",
    "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action",
    "authors": [
      "Zhengyuan Yang",
      "Linjie Li",
      "Jianfeng Wang",
      "Kevin Lin",
      "Ehsan Azarnasab",
      "Faisal Ahmed",
      "Zicheng Liu",
      "Ce Liu",
      "Michael Zeng",
      "Lijuan Wang"
    ],
    "year": 2023,
    "abstract": "We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT's prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT's system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/",
    "url": "http://arxiv.org/abs/2303.11381v1"
  },
  {
    "reference": "Multimodal few-shot learning with frozen language models",
    "title": "Multimodal Few-Shot Learning with Frozen Language Models",
    "authors": [
      "Maria Tsimpoukelli",
      "Jacob Menick",
      "Serkan Cabi",
      "S. M. Ali Eslami",
      "Oriol Vinyals",
      "Felix Hill"
    ],
    "year": 2021,
    "abstract": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
    "url": "http://arxiv.org/abs/2106.13884v2"
  },
  {
    "reference": "BLIP-2 Bootstrapping languageimage pre-training with frozen image encoders and large language models",
    "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
    "authors": [
      "Junnan Li",
      "Dongxu Li",
      "Silvio Savarese",
      "Steven Hoi"
    ],
    "year": 2023,
    "abstract": "The cost of vision-and-language pre-training has become increasingly\nprohibitive due to end-to-end training of large-scale models. This paper\nproposes BLIP-2, a generic and efficient pre-training strategy that bootstraps\nvision-language pre-training from off-the-shelf frozen pre-trained image\nencoders and frozen large language models. BLIP-2 bridges the modality gap with\na lightweight Querying Transformer, which is pre-trained in two stages. The\nfirst stage bootstraps vision-language representation learning from a frozen\nimage encoder. The second stage bootstraps vision-to-language generative\nlearning from a frozen language model. BLIP-2 achieves state-of-the-art\nperformance on various vision-language tasks, despite having significantly\nfewer trainable parameters than existing methods. For example, our model\noutperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable\nparameters. We also demonstrate the model's emerging capabilities of zero-shot\nimage-to-text generation that can follow natural language instructions.",
    "url": "http://arxiv.org/abs/2301.12597v3"
  },
  {
    "reference": "LLaMA-Adapter Efficient fine-tuning of language models with zero-init attention",
    "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
    "authors": [
      "Renrui Zhang",
      "Jiaming Han",
      "Chris Liu",
      "Peng Gao",
      "Aojun Zhou",
      "Xiangfei Hu",
      "Shilin Yan",
      "Pan Lu",
      "Hongsheng Li",
      "Yu Qiao"
    ],
    "year": 2023,
    "abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the word tokens at higher transformer layers. Then, a\nzero-initialized attention mechanism with zero gating is proposed, which\nadaptively injects the new instructional cues into LLaMA, while effectively\npreserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter\ncan generate high-quality responses, comparable to Alpaca with fully fine-tuned\n7B parameters. Besides language commands, our approach can be simply extended\nto multi-modal instructions for learning image-conditioned LLaMA model, which\nachieves superior reasoning performance on ScienceQA and COCO Caption\nbenchmarks. Furthermore, we also evaluate the zero-initialized attention\nmechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on\ntraditional vision and language tasks, demonstrating the superior\ngeneralization capacity of our approach. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.",
    "url": "http://arxiv.org/abs/2303.16199v3"
  },
  {
    "reference": "MiniGPT-4 Enhancing vision-language understanding with advanced large language models",
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "authors": [
      "Deyao Zhu",
      "Jun Chen",
      "Xiaoqian Shen",
      "Xiang Li",
      "Mohamed Elhoseiny"
    ],
    "year": 2023,
    "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such\nas directly generating websites from handwritten text and identifying humorous\nelements within images. These features are rarely observed in previous\nvision-language models. However, the technical details behind GPT-4 continue to\nremain undisclosed. We believe that the enhanced multi-modal generation\ncapabilities of GPT-4 stem from the utilization of sophisticated large language\nmodels (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a\nfrozen visual encoder with a frozen advanced LLM, Vicuna, using one projection\nlayer. Our work, for the first time, uncovers that properly aligning the visual\nfeatures with an advanced large language model can possess numerous advanced\nmulti-modal abilities demonstrated by GPT-4, such as detailed image description\ngeneration and website creation from hand-drawn drafts. Furthermore, we also\nobserve other emerging capabilities in MiniGPT-4, including writing stories and\npoems inspired by given images, teaching users how to cook based on food\nphotos, and so on. In our experiment, we found that the model trained on short\nimage caption pairs could produce unnatural language outputs (e.g., repetition\nand fragmentation). To address this problem, we curate a detailed image\ndescription dataset in the second stage to finetune the model, which\nconsequently improves the model's generation reliability and overall usability.\nOur code, pre-trained model, and collected dataset are available at\nhttps://minigpt-4.github.io/.",
    "url": "http://arxiv.org/abs/2304.10592v2"
  },
  {
    "reference": "Visual instruction tuning",
    "title": "Visual Instruction Tuning",
    "authors": [
      "Haotian Liu",
      "Chunyuan Li",
      "Qingyang Wu",
      "Yong Jae Lee"
    ],
    "year": 2023,
    "abstract": "Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.",
    "url": "http://arxiv.org/abs/2304.08485v2"
  },
  {
    "reference": "An empirical study of GPT-3 for few-shot knowledge-based vqa",
    "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
    "authors": [
      "Zhengyuan Yang",
      "Zhe Gan",
      "Jianfeng Wang",
      "Xiaowei Hu",
      "Yumao Lu",
      "Zicheng Liu",
      "Lijuan Wang"
    ],
    "year": 2021,
    "abstract": "Knowledge-based visual question answering (VQA) involves answering questions\nthat require external knowledge not present in the image. Existing methods\nfirst retrieve knowledge from external resources, then reason over the selected\nknowledge, the input image, and question for answer prediction. However, this\ntwo-step approach could lead to mismatches that potentially limit the VQA\nperformance. For example, the retrieved knowledge might be noisy and irrelevant\nto the question, and the re-embedded knowledge features during reasoning might\ndeviate from their original meanings in the knowledge base (KB). To address\nthis challenge, we propose PICa, a simple yet effective method that Prompts\nGPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by\nGPT-3's power in knowledge retrieval and question answering, instead of using\nstructured KBs as in previous work, we treat GPT-3 as an implicit and\nunstructured KB that can jointly acquire and process relevant knowledge.\nSpecifically, we first convert the image into captions (or tags) that GPT-3 can\nunderstand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just\nproviding a few in-context VQA examples. We further boost performance by\ncarefully investigating: (i) what text formats best describe the image content,\nand (ii) how in-context examples can be better selected and used. PICa unlocks\nthe first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa\nsurpasses the supervised state of the art by an absolute +8.6 points on the\nOK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent\nfew-shot performance.",
    "url": "http://arxiv.org/abs/2109.05014v2"
  },
  {
    "reference": "Plug-and-play VQA Zero-shot vqa by conjoining large pretrained models with zero training",
    "title": "Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training",
    "authors": [
      "Anthony Meng Huat Tiong",
      "Junnan Li",
      "Boyang Li",
      "Silvio Savarese",
      "Steven C. H. Hoi"
    ],
    "year": 2022,
    "abstract": "Visual question answering (VQA) is a hallmark of vision and language\nreasoning and a challenging task under the zero-shot setting. We propose\nPlug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast\nto most existing works, which require substantial adaptation of pretrained\nlanguage models (PLMs) for the vision modality, PNP-VQA requires no additional\ntraining of the PLMs. Instead, we propose to use natural language and network\ninterpretation as an intermediate representation that glues pretrained models\ntogether. We first generate question-guided informative image captions, and\npass the captions to a PLM as context for question answering. Surpassing\nend-to-end trained baselines, PNP-VQA achieves state-of-the-art results on\nzero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter\nFlamingo model by 8.5% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an\nimprovement of 9.1% on GQA over FewVLM with 740M PLM parameters. Code is\nreleased at https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa",
    "url": "http://arxiv.org/abs/2210.08773v3"
  },
  {
    "reference": "From images to textual prompts Zero-shot VQA with frozen large language models",
    "title": "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models",
    "authors": [
      "Jiaxian Guo",
      "Junnan Li",
      "Dongxu Li",
      "Anthony Meng Huat Tiong",
      "Boyang Li",
      "Dacheng Tao",
      "Steven C. H. Hoi"
    ],
    "year": 2022,
    "abstract": "Large language models (LLMs) have demonstrated excellent zero-shot\ngeneralization to new language tasks. However, effective utilization of LLMs\nfor zero-shot visual question-answering (VQA) remains challenging, primarily\ndue to the modality disconnection and task disconnection between LLM and VQA\ntask. End-to-end training on vision and language data may bridge the\ndisconnections, but is inflexible and computationally expensive. To address\nthis issue, we propose \\emph{Img2Prompt}, a plug-and-play module that provides\nthe prompts that can bridge the aforementioned modality and task\ndisconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end\ntraining. In order to provide such prompts, we further employ LLM-agnostic\nmodels to provide prompts that can describe image content and self-constructed\nquestion-answer pairs, which can effectively guide LLM to perform zero-shot VQA\ntasks. Img2Prompt offers the following benefits: 1) It can flexibly work with\nvarious LLMs to perform VQA. 2)~Without the needing of end-to-end training, it\nsignificantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It\nachieves comparable or better performance than methods relying on end-to-end\ntraining. For example, we outperform Flamingo \\cite{Deepmind:Flamingo2022} by\n5.6\\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms\nfew-shot methods by as much as 20\\%.",
    "url": "http://arxiv.org/abs/2212.10846v3"
  },
  {
    "reference": "Captioning images taken by people who are blind",
    "title": "Captioning Images Taken by People Who Are Blind",
    "authors": [
      "Danna Gurari",
      "Yinan Zhao",
      "Meng Zhang",
      "Nilavra Bhattacharya"
    ],
    "year": 2020,
    "abstract": "While an important problem in the vision community is to design algorithms\nthat can automatically caption images, few publicly-available datasets for\nalgorithm development directly address the interests of real users. Observing\nthat people who are blind have relied on (human-based) image captioning\nservices to learn about images they take for nearly a decade, we introduce the\nfirst image captioning dataset to represent this real use case. This new\ndataset, which we call VizWiz-Captions, consists of over 39,000 images\noriginating from people who are blind that are each paired with five captions.\nWe analyze this dataset to (1) characterize the typical captions, (2)\ncharacterize the diversity of content found in the images, and (3) compare its\ncontent to that found in eight popular vision datasets. We also analyze modern\nimage captioning algorithms to identify what makes this new dataset challenging\nfor the vision community. We publicly-share the dataset with captioning\nchallenge instructions at https://vizwiz.org",
    "url": "http://arxiv.org/abs/2002.08565v2"
  },
  {
    "reference": "Photorealistic text-to-image diffusion models with deep language understanding",
    "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
    "authors": [
      "Chitwan Saharia",
      "William Chan",
      "Saurabh Saxena",
      "Lala Li",
      "Jay Whang",
      "Emily Denton",
      "Seyed Kamyar Seyed Ghasemipour",
      "Burcu Karagol Ayan",
      "S. Sara Mahdavi",
      "Rapha Gontijo Lopes",
      "Tim Salimans",
      "Jonathan Ho",
      "David J Fleet",
      "Mohammad Norouzi"
    ],
    "year": 2022,
    "abstract": "We present Imagen, a text-to-image diffusion model with an unprecedented\ndegree of photorealism and a deep level of language understanding. Imagen\nbuilds on the power of large transformer language models in understanding text\nand hinges on the strength of diffusion models in high-fidelity image\ngeneration. Our key discovery is that generic large language models (e.g. T5),\npretrained on text-only corpora, are surprisingly effective at encoding text\nfor image synthesis: increasing the size of the language model in Imagen boosts\nboth sample fidelity and image-text alignment much more than increasing the\nsize of the image diffusion model. Imagen achieves a new state-of-the-art FID\nscore of 7.27 on the COCO dataset, without ever training on COCO, and human\nraters find Imagen samples to be on par with the COCO data itself in image-text\nalignment. To assess text-to-image models in greater depth, we introduce\nDrawBench, a comprehensive and challenging benchmark for text-to-image models.\nWith DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,\nLatent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen\nover other models in side-by-side comparisons, both in terms of sample quality\nand image-text alignment. See https://imagen.research.google/ for an overview\nof the results.",
    "url": "http://arxiv.org/abs/2205.11487v1"
  },
  {
    "reference": "BSL-1K Scaling up co-articulated sign language recognition using mouthing cues",
    "title": "BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues",
    "authors": [
      "Samuel Albanie",
      "G\u00fcl Varol",
      "Liliane Momeni",
      "Triantafyllos Afouras",
      "Joon Son Chung",
      "Neil Fox",
      "Andrew Zisserman"
    ],
    "year": 2020,
    "abstract": "Recent progress in fine-grained gesture and action classification, and\nmachine translation, point to the possibility of automated sign language\nrecognition becoming a reality. A key stumbling block in making progress\ntowards this goal is a lack of appropriate training data, stemming from the\nhigh complexity of sign annotation and a limited supply of qualified\nannotators. In this work, we introduce a new scalable approach to data\ncollection for sign recognition in continuous videos. We make use of\nweakly-aligned subtitles for broadcast footage together with a keyword spotting\nmethod to automatically localise sign-instances for a vocabulary of 1,000 signs\nin 1,000 hours of video. We make the following contributions: (1) We show how\nto use mouthing cues from signers to obtain high-quality annotations from video\ndata - the result is the BSL-1K dataset, a collection of British Sign Language\n(BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to train\nstrong sign recognition models for co-articulated signs in BSL and that these\nmodels additionally form excellent pretraining for other sign languages and\nbenchmarks - we exceed the state of the art on both the MSASL and WLASL\nbenchmarks. Finally, (3) we propose new large-scale evaluation sets for the\ntasks of sign recognition and sign spotting and provide baselines which we hope\nwill serve to stimulate research in this area.",
    "url": "http://arxiv.org/abs/2007.12131v2"
  },
  {
    "reference": "Emotion recognition from multiple modalities Fundamentals and methodologies",
    "title": "Emotion Recognition from Multiple Modalities: Fundamentals and Methodologies",
    "authors": [
      "Sicheng Zhao",
      "Guoli Jia",
      "Jufeng Yang",
      "Guiguang Ding",
      "Kurt Keutzer"
    ],
    "year": 2021,
    "abstract": "Humans are emotional creatures. Multiple modalities are often involved when\nwe express emotions, whether we do so explicitly (e.g., facial expression,\nspeech) or implicitly (e.g., text, image). Enabling machines to have emotional\nintelligence, i.e., recognizing, interpreting, processing, and simulating\nemotions, is becoming increasingly important. In this tutorial, we discuss\nseveral key aspects of multi-modal emotion recognition (MER). We begin with a\nbrief introduction on widely used emotion representation models and affective\nmodalities. We then summarize existing emotion annotation strategies and\ncorresponding computational tasks, followed by the description of main\nchallenges in MER. Furthermore, we present some representative approaches on\nrepresentation learning of each affective modality, feature fusion of different\naffective modalities, classifier optimization for MER, and domain adaptation\nfor MER. Finally, we outline several real-world applications and discuss some\nfuture directions.",
    "url": "http://arxiv.org/abs/2108.10152v1"
  },
  {
    "reference": "VideoCLIP Contrastive pretraining for zero-shot video-text understanding",
    "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
    "authors": [
      "Hu Xu",
      "Gargi Ghosh",
      "Po-Yao Huang",
      "Dmytro Okhonko",
      "Armen Aghajanyan",
      "Florian Metze",
      "Luke Zettlemoyer",
      "Christoph Feichtenhofer"
    ],
    "year": 2021,
    "abstract": "We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/tree/main/examples/MMPT.",
    "url": "http://arxiv.org/abs/2109.14084v2"
  },
  {
    "reference": "mPLUG-2 A modularized multi-modal foundation model across text image and video",
    "title": "mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
    "authors": [
      "Haiyang Xu",
      "Qinghao Ye",
      "Ming Yan",
      "Yaya Shi",
      "Jiabo Ye",
      "Yuanhong Xu",
      "Chenliang Li",
      "Bin Bi",
      "Qi Qian",
      "Wei Wang",
      "Guohai Xu",
      "Ji Zhang",
      "Songfang Huang",
      "Fei Huang",
      "Jingren Zhou"
    ],
    "year": 2023,
    "abstract": "Recent years have witnessed a big convergence of language, vision, and\nmulti-modal pretraining. In this work, we present mPLUG-2, a new unified\nparadigm with modularized design for multi-modal pretraining, which can benefit\nfrom modality collaboration while addressing the problem of modality\nentanglement. In contrast to predominant paradigms of solely relying on\nsequence-to-sequence generation or encoder-based instance discrimination,\nmPLUG-2 introduces a multi-module composition network by sharing common\nuniversal modules for modality collaboration and disentangling different\nmodality modules to deal with modality entanglement. It is flexible to select\ndifferent modules for different understanding and generation tasks across all\nmodalities including text, image, and video. Empirical study shows that mPLUG-2\nachieves state-of-the-art or competitive results on a broad range of over 30\ndownstream tasks, spanning multi-modal tasks of image-text and video-text\nunderstanding and generation, and uni-modal tasks of text-only, image-only, and\nvideo-only understanding. Notably, mPLUG-2 shows new state-of-the-art results\nof 48.0 top-1 accuracy and 80.3 CIDEr on the challenging MSRVTT video QA and\nvideo caption tasks with a far smaller model size and data scale. It also\ndemonstrates strong zero-shot transferability on vision-language and\nvideo-language tasks. Code and models will be released in\nhttps://github.com/alibaba/AliceMind.",
    "url": "http://arxiv.org/abs/2302.00402v1"
  },
  {
    "reference": "MusicLM Generating music from text",
    "title": "MusicLM: Generating Music From Text",
    "authors": [
      "Andrea Agostinelli",
      "Timo I. Denk",
      "Zal\u00e1n Borsos",
      "Jesse Engel",
      "Mauro Verzetti",
      "Antoine Caillon",
      "Qingqing Huang",
      "Aren Jansen",
      "Adam Roberts",
      "Marco Tagliasacchi",
      "Matt Sharifi",
      "Neil Zeghidour",
      "Christian Frank"
    ],
    "year": 2023,
    "abstract": "We introduce MusicLM, a model generating high-fidelity music from text\ndescriptions such as \"a calming violin melody backed by a distorted guitar\nriff\". MusicLM casts the process of conditional music generation as a\nhierarchical sequence-to-sequence modeling task, and it generates music at 24\nkHz that remains consistent over several minutes. Our experiments show that\nMusicLM outperforms previous systems both in audio quality and adherence to the\ntext description. Moreover, we demonstrate that MusicLM can be conditioned on\nboth text and a melody in that it can transform whistled and hummed melodies\naccording to the style described in a text caption. To support future research,\nwe publicly release MusicCaps, a dataset composed of 5.5k music-text pairs,\nwith rich text descriptions provided by human experts.",
    "url": "http://arxiv.org/abs/2301.11325v1"
  },
  {
    "reference": "Meshtalk 3D face animation from speech using cross-modality disentanglement",
    "title": "MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement",
    "authors": [
      "Alexander Richard",
      "Michael Zollhoefer",
      "Yandong Wen",
      "Fernando de la Torre",
      "Yaser Sheikh"
    ],
    "year": 2021,
    "abstract": "This paper presents a generic method for generating full facial 3D animation\nfrom speech. Existing approaches to audio-driven facial animation exhibit\nuncanny or static upper face animation, fail to produce accurate and plausible\nco-articulation or rely on person-specific models that limit their scalability.\nTo improve upon existing models, we propose a generic audio-driven facial\nanimation approach that achieves highly realistic motion synthesis results for\nthe entire face. At the core of our approach is a categorical latent space for\nfacial animation that disentangles audio-correlated and audio-uncorrelated\ninformation based on a novel cross-modality loss. Our approach ensures highly\naccurate lip motion, while also synthesizing plausible animation of the parts\nof the face that are uncorrelated to the audio signal, such as eye blinks and\neye brow motion. We demonstrate that our approach outperforms several baselines\nand obtains state-of-the-art quality both qualitatively and quantitatively. A\nperceptual user study demonstrates that our approach is deemed more realistic\nthan the current state-of-the-art in over 75% of cases. We recommend watching\nthe supplemental video before reading the paper:\nhttps://github.com/facebookresearch/meshtalk",
    "url": "http://arxiv.org/abs/2104.08223v2"
  },
  {
    "reference": "AI-generated content AIGC A survey",
    "title": "AI-Generated Content (AIGC): A Survey",
    "authors": [
      "Jiayang Wu",
      "Wensheng Gan",
      "Zefeng Chen",
      "Shicheng Wan",
      "Hong Lin"
    ],
    "year": 2023,
    "abstract": "To address the challenges of digital intelligence in the digital economy,\nartificial intelligence-generated content (AIGC) has emerged. AIGC uses\nartificial intelligence to assist or replace manual content generation by\ngenerating content based on user-inputted keywords or requirements. The\ndevelopment of large model algorithms has significantly strengthened the\ncapabilities of AIGC, which makes AIGC products a promising generative tool and\nadds convenience to our lives. As an upstream technology, AIGC has unlimited\npotential to support different downstream applications. It is important to\nanalyze AIGC's current capabilities and shortcomings to understand how it can\nbe best utilized in future applications. Therefore, this paper provides an\nextensive overview of AIGC, covering its definition, essential conditions,\ncutting-edge capabilities, and advanced features. Moreover, it discusses the\nbenefits of large-scale pre-trained models and the industrial chain of AIGC.\nFurthermore, the article explores the distinctions between auxiliary generation\nand automatic generation within AIGC, providing examples of text generation.\nThe paper also examines the potential integration of AIGC with the Metaverse.\nLastly, the article highlights existing issues and suggests some future\ndirections for application.",
    "url": "http://arxiv.org/abs/2304.06632v1"
  },
  {
    "reference": "A lip sync expert is all you need for speech to lip generation in the wild",
    "title": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild",
    "authors": [
      "K R Prajwal",
      "Rudrabha Mukhopadhyay",
      "Vinay Namboodiri",
      "C V Jawahar"
    ],
    "year": 2020,
    "abstract": "In this work, we investigate the problem of lip-syncing a talking face video\nof an arbitrary identity to match a target speech segment. Current works excel\nat producing accurate lip movements on a static image or videos of specific\npeople seen during the training phase. However, they fail to accurately morph\nthe lip movements of arbitrary identities in dynamic, unconstrained talking\nface videos, resulting in significant parts of the video being out-of-sync with\nthe new audio. We identify key reasons pertaining to this and hence resolve\nthem by learning from a powerful lip-sync discriminator. Next, we propose new,\nrigorous evaluation benchmarks and metrics to accurately measure lip\nsynchronization in unconstrained videos. Extensive quantitative evaluations on\nour challenging benchmarks show that the lip-sync accuracy of the videos\ngenerated by our Wav2Lip model is almost as good as real synced videos. We\nprovide a demo video clearly showing the substantial impact of our Wav2Lip\nmodel and evaluation benchmarks on our website:\n\\url{cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild}.\nThe code and models are released at this GitHub repository:\n\\url{github.com/Rudrabha/Wav2Lip}. You can also try out the interactive demo at\nthis link: \\url{bhaasha.iiit.ac.in/lipsync}.",
    "url": "http://arxiv.org/abs/2008.10010v1"
  },
  {
    "reference": "Microsoft COCO Common objects in context",
    "title": "Microsoft COCO: Common Objects in Context",
    "authors": [
      "Tsung-Yi Lin",
      "Michael Maire",
      "Serge Belongie",
      "Lubomir Bourdev",
      "Ross Girshick",
      "James Hays",
      "Pietro Perona",
      "Deva Ramanan",
      "C. Lawrence Zitnick",
      "Piotr Doll\u00e1r"
    ],
    "year": 2014,
    "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in\nobject recognition by placing the question of object recognition in the context\nof the broader question of scene understanding. This is achieved by gathering\nimages of complex everyday scenes containing common objects in their natural\ncontext. Objects are labeled using per-instance segmentations to aid in precise\nobject localization. Our dataset contains photos of 91 objects types that would\nbe easily recognizable by a 4 year old. With a total of 2.5 million labeled\ninstances in 328k images, the creation of our dataset drew upon extensive crowd\nworker involvement via novel user interfaces for category detection, instance\nspotting and instance segmentation. We present a detailed statistical analysis\nof the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide\nbaseline performance analysis for bounding box and segmentation detection\nresults using a Deformable Parts Model.",
    "url": "http://arxiv.org/abs/1405.0312v3"
  },
  {
    "reference": "Visual genome Connecting language and vision using crowdsourced dense image annotations",
    "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
    "authors": [
      "Ranjay Krishna",
      "Yuke Zhu",
      "Oliver Groth",
      "Justin Johnson",
      "Kenji Hata",
      "Joshua Kravitz",
      "Stephanie Chen",
      "Yannis Kalantidis",
      "Li-Jia Li",
      "David A. Shamma",
      "Michael S. Bernstein",
      "Fei-Fei Li"
    ],
    "year": 2016,
    "abstract": "Despite progress in perceptual tasks such as image classification, computers\nstill perform poorly on cognitive tasks such as image description and question\nanswering. Cognition is core to tasks that involve not just recognizing, but\nreasoning about our visual world. However, models used to tackle the rich\ncontent in images for cognitive tasks are still being trained using the same\ndatasets designed for perceptual tasks. To achieve success at cognitive tasks,\nmodels need to understand the interactions and relationships between objects in\nan image. When asked \"What vehicle is the person riding?\", computers will need\nto identify the objects in an image as well as the relationships riding(man,\ncarriage) and pulling(horse, carriage) in order to answer correctly that \"the\nperson is riding a horse-drawn carriage\".\n  In this paper, we present the Visual Genome dataset to enable the modeling of\nsuch relationships. We collect dense annotations of objects, attributes, and\nrelationships within each image to learn these models. Specifically, our\ndataset contains over 100K images where each image has an average of 21\nobjects, 18 attributes, and 18 pairwise relationships between objects. We\ncanonicalize the objects, attributes, relationships, and noun phrases in region\ndescriptions and questions answer pairs to WordNet synsets. Together, these\nannotations represent the densest and largest dataset of image descriptions,\nobjects, attributes, relationships, and question answers.",
    "url": "http://arxiv.org/abs/1602.07332v1"
  },
  {
    "reference": "Towards automatic learning of procedures from web instructional videos",
    "title": "Towards Automatic Learning of Procedures from Web Instructional Videos",
    "authors": [
      "Luowei Zhou",
      "Chenliang Xu",
      "Jason J. Corso"
    ],
    "year": 2017,
    "abstract": "The potential for agents, whether embodied or software, to learn by observing\nother agents performing procedures involving objects and actions is rich.\nCurrent research on automatic procedure learning heavily relies on action\nlabels or video subtitles, even during the evaluation phase, which makes them\ninfeasible in real-world scenarios. This leads to our question: can the\nhuman-consensus structure of a procedure be learned from a large set of long,\nunconstrained videos (e.g., instructional videos from YouTube) with only visual\nevidence? To answer this question, we introduce the problem of procedure\nsegmentation--to segment a video procedure into category-independent procedure\nsegments. Given that no large-scale dataset is available for this problem, we\ncollect a large-scale procedure segmentation dataset with procedure segments\ntemporally localized and described; we use cooking videos and name the dataset\nYouCook2. We propose a segment-level recurrent network for generating procedure\nsegments by modeling the dependencies across segments. The generated segments\ncan be used as pre-processing for other tasks, such as dense video captioning\nand event parsing. We show in our experiments that the proposed model\noutperforms competitive baselines in procedure segmentation.",
    "url": "http://arxiv.org/abs/1703.09788v3"
  },
  {
    "reference": "Frozen in time A joint video and image encoder for end-to-end retrieval",
    "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
    "authors": [
      "Max Bain",
      "Arsha Nagrani",
      "G\u00fcl Varol",
      "Andrew Zisserman"
    ],
    "year": 2021,
    "abstract": "Our objective in this work is video-text retrieval - in particular a joint\nembedding that enables efficient text-to-video retrieval. The challenges in\nthis area include the design of the visual architecture and the nature of the\ntraining data, in that the available large scale video-text training datasets,\nsuch as HowTo100M, are noisy and hence competitive performance is achieved only\nat scale through large amounts of compute. We address both these challenges in\nthis paper. We propose an end-to-end trainable model that is designed to take\nadvantage of both large-scale image and video captioning datasets. Our model is\nan adaptation and extension of the recent ViT and Timesformer architectures,\nand consists of attention in both space and time. The model is flexible and can\nbe trained on both image and video text datasets, either independently or in\nconjunction. It is trained with a curriculum learning schedule that begins by\ntreating images as 'frozen' snapshots of video, and then gradually learns to\nattend to increasing temporal context when trained on video datasets. We also\nprovide a new video-text pretraining dataset WebVid-2M, comprised of over two\nmillion videos with weak captions scraped from the internet. Despite training\non datasets that are an order of magnitude smaller, we show that this approach\nyields state-of-the-art results on standard downstream video-retrieval\nbenchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.",
    "url": "http://arxiv.org/abs/2104.00650v2"
  },
  {
    "reference": "Common voice A massively-multilingual speech corpus",
    "title": "Common Voice: A Massively-Multilingual Speech Corpus",
    "authors": [
      "Rosana Ardila",
      "Megan Branson",
      "Kelly Davis",
      "Michael Henretty",
      "Michael Kohler",
      "Josh Meyer",
      "Reuben Morais",
      "Lindsay Saunders",
      "Francis M. Tyers",
      "Gregor Weber"
    ],
    "year": 2019,
    "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed\nspeech intended for speech technology research and development. Common Voice is\ndesigned for Automatic Speech Recognition purposes but can be useful in other\ndomains (e.g. language identification). To achieve scale and sustainability,\nthe Common Voice project employs crowdsourcing for both data collection and\ndata validation. The most recent release includes 29 languages, and as of\nNovember 2019 there are a total of 38 languages collecting data. Over 50,000\nindividuals have participated so far, resulting in 2,500 hours of collected\naudio. To our knowledge this is the largest audio corpus in the public domain\nfor speech recognition, both in terms of number of hours and number of\nlanguages. As an example use case for Common Voice, we present speech\nrecognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By\napplying transfer learning from a source English model, we find an average\nCharacter Error Rate improvement of 5.99 +/- 5.48 for twelve target languages\n(German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton,\nTatar, Chuvash, and Kabyle). For most of these languages, these are the first\never published results on end-to-end Automatic Speech Recognition.",
    "url": "http://arxiv.org/abs/1912.06670v2"
  },
  {
    "reference": "Continual learning through synaptic intelligence",
    "title": "Continual Learning Through Synaptic Intelligence",
    "authors": [
      "Friedemann Zenke",
      "Ben Poole",
      "Surya Ganguli"
    ],
    "year": 2017,
    "abstract": "While deep learning has led to remarkable advances across diverse\napplications, it struggles in domains where the data distribution changes over\nthe course of learning. In stark contrast, biological neural networks\ncontinually adapt to changing domains, possibly by leveraging complex molecular\nmachinery to solve many tasks simultaneously. In this study, we introduce\nintelligent synapses that bring some of this biological complexity into\nartificial neural networks. Each synapse accumulates task relevant information\nover time, and exploits this information to rapidly store new memories without\nforgetting old ones. We evaluate our approach on continual learning of\nclassification tasks, and show that it dramatically reduces forgetting while\nmaintaining computational efficiency.",
    "url": "http://arxiv.org/abs/1703.04200v3"
  },
  {
    "reference": "Large language models for robotics A survey",
    "title": "Large Language Models for Robotics: A Survey",
    "authors": [
      "Fanlong Zeng",
      "Wensheng Gan",
      "Yongheng Wang",
      "Ning Liu",
      "Philip S. Yu"
    ],
    "year": 2023,
    "abstract": "The human ability to learn, generalize, and control complex manipulation\ntasks through multi-modality feedback suggests a unique capability, which we\nrefer to as dexterity intelligence. Understanding and assessing this\nintelligence is a complex task. Amidst the swift progress and extensive\nproliferation of large language models (LLMs), their applications in the field\nof robotics have garnered increasing attention. LLMs possess the ability to\nprocess and generate natural language, facilitating efficient interaction and\ncollaboration with robots. Researchers and engineers in the field of robotics\nhave recognized the immense potential of LLMs in enhancing robot intelligence,\nhuman-robot interaction, and autonomy. Therefore, this comprehensive review\naims to summarize the applications of LLMs in robotics, delving into their\nimpact and contributions to key areas such as robot control, perception,\ndecision-making, and path planning. We first provide an overview of the\nbackground and development of LLMs for robotics, followed by a description of\nthe benefits of LLMs for robotics and recent advancements in robotics models\nbased on LLMs. We then delve into the various techniques used in the model,\nincluding those employed in perception, decision-making, control, and\ninteraction. Finally, we explore the applications of LLMs in robotics and some\npotential challenges they may face in the near future. Embodied intelligence is\nthe future of intelligent science, and LLMs-based robotics is one of the\npromising but challenging paths to achieve this.",
    "url": "http://arxiv.org/abs/2311.07226v1"
  },
  {
    "reference": "PaLI A jointly-scaled multilingual language-image model",
    "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
    "authors": [
      "Xi Chen",
      "Xiao Wang",
      "Soravit Changpinyo",
      "AJ Piergiovanni",
      "Piotr Padlewski",
      "Daniel Salz",
      "Sebastian Goodman",
      "Adam Grycner",
      "Basil Mustafa",
      "Lucas Beyer",
      "Alexander Kolesnikov",
      "Joan Puigcerver",
      "Nan Ding",
      "Keran Rong",
      "Hassan Akbari",
      "Gaurav Mishra",
      "Linting Xue",
      "Ashish Thapliyal",
      "James Bradbury",
      "Weicheng Kuo",
      "Mojtaba Seyedhosseini",
      "Chao Jia",
      "Burcu Karagol Ayan",
      "Carlos Riquelme",
      "Andreas Steiner",
      "Anelia Angelova",
      "Xiaohua Zhai",
      "Neil Houlsby",
      "Radu Soricut"
    ],
    "year": 2022,
    "abstract": "Effective scaling and a flexible task interface enable large language models\nto excel at many tasks. We present PaLI (Pathways Language and Image model), a\nmodel that extends this approach to the joint modeling of language and vision.\nPaLI generates text based on visual and textual inputs, and with this interface\nperforms many vision, language, and multimodal tasks, in many languages. To\ntrain PaLI, we make use of large pre-trained encoder-decoder language models\nand Vision Transformers (ViTs). This allows us to capitalize on their existing\ncapabilities and leverage the substantial cost of training them. We find that\njoint scaling of the vision and language components is important. Since\nexisting Transformers for language are much larger than their vision\ncounterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the\nbenefits from even larger-capacity vision models. To train PaLI, we create a\nlarge multilingual mix of pretraining tasks, based on a new image-text training\nset containing 10B images and texts in over 100 languages. PaLI achieves\nstate-of-the-art in multiple vision and language tasks (such as captioning,\nvisual question-answering, scene-text understanding), while retaining a simple,\nmodular, and scalable design.",
    "url": "http://arxiv.org/abs/2209.06794v4"
  }
]