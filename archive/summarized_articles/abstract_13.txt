The paper investigates the performance of large language models, specifically a 3-billion parameter model, in the legal case entailment task, part of the COLIEE 2022 competition. The authors demonstrate that larger models, like their monoT5-3b, outperform smaller models and even those fine-tuned on in-domain data, highlighting the zero-shot capabilities of such models. They show that scaling up model parameters significantly enhances performance, as evidenced by their model achieving top results in the COLIEE 2021 and 2022 competitions. The study emphasizes that large language models can generalize well without domain-specific fine-tuning, which is particularly beneficial in fields like legal NLP where annotated data is scarce. The authors also discuss their method for answer selection and ensemble approaches to further improve performance. Despite the advantages, they acknowledge the challenge of deploying large models due to latency and computational demands, suggesting the need for optimization techniques. The paper concludes that while large models offer substantial benefits in zero-shot learning scenarios, particularly in data-limited domains, practical deployment challenges remain due to their size and resource requirements.