The paper introduces AraLegal-BERT, a specialized BERT-based model optimized for Arabic legal texts. Recognizing the limitations of general BERT models in domain-specific tasks, the authors developed AraLegal-BERT by pre-training it from scratch using a dataset of 13.7 million sentences from various legal documents, including legislative texts and judicial rulings. The model was tested against existing Arabic BERT variations on three Natural Language Understanding (NLU) tasks: legal text classification, keyword extraction, and named entity recognition. AraLegal-BERT outperformed other models in these tasks, demonstrating higher accuracy and efficiency, particularly in handling domain-specific vocabulary and semantics. The study highlights the benefits of domain-specific pre-training, showing that AraLegal-BERT can achieve better performance with less computational resources compared to larger, generalist models. The paper concludes with a suggestion for future work to explore pre-training other models like Electra and Roberta for Arabic legal texts.