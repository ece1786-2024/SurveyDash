The paper introduces mT5, a multilingual variant of the T5 model, pre-trained on a new dataset called mC4, which includes data from 101 languages sourced from Common Crawl. The mT5 model retains the text-to-text format of T5, allowing it to handle various NLP tasks uniformly. It achieves state-of-the-art results on several multilingual benchmarks. The paper also addresses the issue of "accidental translation" in zero-shot settings, where the model might incorrectly translate predictions into another language. A simple technique involving mixing unlabeled pre-training data during fine-tuning is proposed to mitigate this issue. The mC4 dataset expands the C4 dataset to multiple languages, employing a filtering process to ensure quality data. The mT5 model architecture follows the T5.1.1 recipe, with adjustments for multilinguality, such as a larger vocabulary. The model is compared to other multilingual models like mBERT and XLM-R, demonstrating superior performance on cross-lingual tasks. Experiments validate mT5's effectiveness across tasks in the XTREME benchmark, with results showing that larger models close the performance gap with monolingual models. The paper also explores the challenges of zero-shot generation, proposing domain-preserving training to prevent illegal predictions. The authors release the code and pre-trained models to support further research in multilingual NLP.