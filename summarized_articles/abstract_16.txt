The paper introduces Clue And Reasoning Prompting (CARP), a framework designed to enhance text classification using large language models (LLMs) by addressing their limitations in reasoning and token usage. CARP improves upon LLMs by first prompting them to identify superficial clues in the text, which are then used to perform a diagnostic reasoning process, leading to more accurate text classification. This method is particularly effective in dealing with complex linguistic phenomena such as irony and negation. CARP also employs a fine-tuned model for kNN demonstration search to maximize the use of training data despite token limitations in LLMs. The framework demonstrated superior performance on four out of five text classification benchmarks, achieving state-of-the-art results on SST-2, AGNews, R8, and R52 datasets, and comparable results on the MR dataset. CARP also showed remarkable performance in low-resource settings, performing comparably to supervised models with significantly fewer examples. The paper discusses related work in LLMs, in-context learning, and text classification, and details the prompt construction and demonstration sampling strategies used in CARP. Experiments confirmed CARP's effectiveness, particularly in low-resource and domain adaptation scenarios, and the paper concludes with suggestions for future research directions.