The paper investigates the zero-shot capabilities of large language models in the legal case entailment task, specifically within the context of the COLIEE 2022 competition. It highlights that models with billions of parameters, such as a 3B-parameter model, outperform smaller models and even some ensembles, indicating that larger models possess superior generalization abilities without the need for domain-specific fine-tuning. The study builds on previous work showing that zero-shot models can surpass fine-tuned models in legal tasks due to the limited availability of annotated legal data. The authors use the monoT5 model, adapted from the T5 model, which is fine-tuned on a general domain dataset (MS MARCO) and evaluated directly on legal texts. They demonstrate the effectiveness of their approach by achieving high F1 scores in the COLIEE 2021 and 2022 test sets, with the 3B-parameter model showing significant performance gains. The paper also explores ensemble methods to further enhance performance. However, the authors acknowledge the challenge of deploying large models in real-time applications due to latency issues, suggesting a need for optimization techniques. The study concludes that scaling language models improves zero-shot performance and could be beneficial for other legal tasks, despite the computational challenges involved.