The paper introduces mT5, a multilingual variant of the T5 model, pre-trained on a new dataset called mC4, which covers 101 languages. This model follows the T5 framework, using a text-to-text format for all tasks, and is trained on a large multilingual dataset derived from Common Crawl. The mC4 dataset is an extension of the C4 dataset, with modifications to accommodate multilingual text, including a new filtering method and language detection. The mT5 model architecture closely mirrors T5, with adaptations to handle multilingual data, such as an increased vocabulary size and language sampling strategy to balance low- and high-resource languages. The paper compares mT5 with other multilingual models, highlighting its state-of-the-art performance on several multilingual benchmarks. Experiments demonstrate mT5's effectiveness across various tasks, including zero-shot, translate-train, and in-language multitask settings, showing that larger model sizes close the performance gap between multilingual and monolingual models. The paper also addresses the issue of "accidental translation" in zero-shot settings, proposing a method to mix multilingual pre-training data during fine-tuning to mitigate this problem. The results show significant improvements in reducing illegal predictions, particularly for smaller models. The authors release all code and pre-trained models to support further research in multilingual NLP.