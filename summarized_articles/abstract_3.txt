The paper provides a comprehensive survey of Legal Judgment Prediction (LJP), which involves using Natural Language Processing (NLP) to predict legal outcomes from case descriptions. It reviews 31 datasets in six languages, categorizing them by task, legal system, and domain. It also summarizes 14 evaluation metrics across four categories and reviews 12 legal-domain pretrained models. The survey highlights three research directions: multi-task learning, interpretable learning, and few-shot learning. Challenges include handling inadequate information, complex legal reasoning, and improving model interpretability. The paper discusses the evolution of LJP from early statistical methods to advanced neural networks and transformer-based models, emphasizing the importance of sophisticated datasets and the need for models that can handle complex legal reasoning and provide interpretable results. The survey concludes with recommendations for future datasets and models, suggesting the incorporation of evidence admissibility, complex reasoning features, and task-specific features to enhance performance.