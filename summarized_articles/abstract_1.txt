The paper presents AraLegal-BERT, a BERT-based model specifically pre-trained for Arabic legal texts, aiming to improve natural language processing (NLP) tasks in this domain. The authors highlight the need for domain-specific adaptations of BERT, as general models often fail to capture specialized terminology effectively. AraLegal-BERT was trained from scratch using a manually collected dataset of 4.5GB, covering various legal sub-fields. The model was evaluated against three Arabic BERT variations on tasks like legal text classification, keyword extraction, and named entity recognition (NER). AraLegal-BERT outperformed other models, showing significant improvements in F1-macro scores, particularly in keyword extraction and NER tasks. The study emphasizes the efficiency of domain-specific pre-training, as AraLegal-BERT achieved high accuracy with less computational demand. Future work will explore pre-training other models like Electra and Roberta for Arabic legal texts. The research was funded by Elm Company.