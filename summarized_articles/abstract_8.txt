The paper explores crosslingual generalization in large language models through multitask finetuning (MTF), focusing on multilingual models like BLOOM and mT5. The authors develop finetuned variants called BLOOMZ and mT0 by applying MTF with English prompts to these models. They find that finetuning on English tasks allows these models to generalize to non-English tasks and languages present in the pretraining corpus, achieving state-of-the-art zero-shot results. The study extends the P3 dataset to xP3, incorporating 46 languages and additional tasks, which improves performance on both English and non-English tasks. The paper also investigates the use of machine-translated prompts (xP3mt), showing improved performance on human-written non-English prompts, albeit at the cost of English prompt performance. The authors conduct a detailed analysis of task and language generalization, scaling effects, and performance on generative tasks, noting that multitask finetuning biases models towards short responses, which can hinder performance on tasks requiring longer outputs. They propose forcing a minimum generation length as a workaround. The study highlights the models' ability to generalize to languages and tasks not intentionally seen during training, suggesting that models learn language- and task-agnostic capabilities. The paper concludes by releasing all datasets and models to aid future research in zero-shot generalization. Limitations include the unnatural prompting format, limited language coverage in xP3, and performance issues common in large language models. Future work is suggested to explore better base models and the potential for learning new languages during finetuning.