The paper "Crosslingual Generalization through Multitask Finetuning" explores the application of multitask prompted finetuning (MTF) on multilingual language models, specifically BLOOM and mT5, to enhance zero-shot task generalization across languages. The study introduces BLOOMZ and mT0, finetuned variants of these models, and demonstrates that finetuning on English tasks with English prompts enables crosslingual task generalization to non-English languages present in the pretraining corpus. Further improvements are observed when finetuning includes multilingual tasks with English prompts, achieving state-of-the-art zero-shot results. The research also examines the impact of using machine-translated prompts in multilingual finetuning, finding that it enhances performance on human-written prompts in respective languages. The models show surprising capabilities of zero-shot generalization to tasks in languages they have never intentionally encountered, suggesting the learning of higher-level, language-agnostic capabilities. The paper introduces xP3, a dataset composite of 46 languages with English and machine-translated prompts, which improves performance on both English and non-English tasks. The study highlights the benefits of multitask finetuning in expanding the applicability of language models to low-resource languages without additional finetuning, while also noting limitations such as the unnatural prompting format and limited language coverage in xP3. The authors release all datasets and models to support further research in improving zero-shot generalization.