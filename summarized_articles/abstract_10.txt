The paper explores the use of Large Language Models (LLMs) for topic classification in public affairs documents, emphasizing their importance for transparency and informed decision-making. It introduces a system comprising modules for harvesting, document layout analysis, and text processing to monitor and classify public affairs documents. The study uses a regex-powered tool to compile a dataset of over 33,000 Spanish legislative documents annotated across 30 topics, demonstrating the potential of LLMs in handling complex language and multi-label classification tasks. The research evaluates four Spanish LLMs, including RoBERTa and GPT2, using different classifiers, and finds that combining LLMs with SVM classifiers achieves high accuracy, even for less frequent topics. The methodology involves breaking down the classification task into binary detection tasks for each topic, leveraging transformer models as backbones. The study highlights the effectiveness of RoBERTa-base with SVM for this task, outperforming other configurations, and notes the challenge of class imbalance. Future work will address biases, explore newer LLMs, and incorporate additional NLP tasks to enhance the analysis of public affairs documents. The research is supported by VINCES Consulting and various fellowships.